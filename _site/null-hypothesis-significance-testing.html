<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Null-hypothesis significance testing | Bayes rule</title>
  <meta name="description" content="An introduction to Bayesian Statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Null-hypothesis significance testing | Bayes rule" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introduction to Bayesian Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Null-hypothesis significance testing | Bayes rule" />
  
  <meta name="twitter:description" content="An introduction to Bayesian Statistics" />
  

<meta name="author" content="Dr Lincoln Colling" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="criticisms-of-p-values.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Stats</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>1</b> Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="1.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability-and-p-values"><i class="fa fa-check"></i><b>1.2</b> Probability and <em>p</em> values</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#understanding-the-p-through-simulation"><i class="fa fa-check"></i><b>1.2.1</b> Understanding the <em>p</em> through simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#a-short-note-on-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> A short note on confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html"><i class="fa fa-check"></i><b>2</b> Criticisms of <em>p</em> values</a>
<ul>
<li class="chapter" data-level="2.1" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#same-measurements-from-different-devices"><i class="fa fa-check"></i><b>2.1</b> Same measurements from different devices</a></li>
<li class="chapter" data-level="2.2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#the-universe-of-possible-events"><i class="fa fa-check"></i><b>2.2</b> The universe of possible events</a></li>
<li class="chapter" data-level="2.3" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html"><i class="fa fa-check"></i><b>3</b> An alternative to <em>p</em> values</a>
<ul>
<li class="chapter" data-level="3.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#doing-inference-with-likelihoods"><i class="fa fa-check"></i><b>3.1</b> Doing inference with likelihoods</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-brief-detour-back-to-sampling-rules"><i class="fa fa-check"></i><b>3.1.1</b> A brief detour back to sampling rules</a></li>
<li class="chapter" data-level="3.1.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#the-likelihood-ratio"><i class="fa fa-check"></i><b>3.1.2</b> The likelihood ratio</a></li>
<li class="chapter" data-level="3.1.3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-note-about-likelihood-functions-and-probability-distributions"><i class="fa fa-check"></i><b>3.1.3</b> A note about likelihood functions and probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#testing-more-complex-hypotheses"><i class="fa fa-check"></i><b>3.2</b> Testing more complex hypotheses</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#theres-more-than-one-way-to-average"><i class="fa fa-check"></i><b>3.2.1</b> There’s more than one way to average</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html"><i class="fa fa-check"></i><b>4</b> The Bayes factor</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay"><i class="fa fa-check"></i><b>4.1</b> Computing Bayes factors with <code>bayesplay</code></a></li>
<li class="chapter" data-level="4.2" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay-web"><i class="fa fa-check"></i><b>4.2</b> Computing Bayes factors with Bayesplay-Web</a></li>
<li class="chapter" data-level="4.3" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#moving-beyond-coin-flips"><i class="fa fa-check"></i><b>4.3</b> Moving beyond coin flips</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="./">Lincoln Colling</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayes rule</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="null-hypothesis-significance-testing" class="section level1" number="1">
<h1><span class="header-section-number">Chapter 1</span> Null-hypothesis significance testing</h1>
<p>Before diving into Bayesian hypothesis testing, it’s worth spending a little
time going over Frequentist null-hypothesis significance testing. The reason
for this is that I want it to be clear that Bayesian methods and Frequentist
methods aren’t just two different ways of answering the same question. Rather,
I want it to be clear that Bayesian methods and Frequentist methods are asking
<strong>fundamentally different questions</strong>. For this reason, we’ll start right back
at the beginning with <em>p</em> values.</p>
<p>The American Statistical Association (ASA) defines a <em>p</em> value as:</p>
<blockquote>
<p>the probability under a specified statistical model that a statistical
summary of the data (e.g., the sample mean difference between two compared
groups) would be equal to or more extreme than its observed value
(<a href="https://doi.org/10.1080/00031305.2016.1154108">Wasserstein and Lazar 2016</a>)</p>
</blockquote>
<p>While this is a perfectly acceptable definition, it is maybe a little tricky to
understand. The main reason for this is that the definition contains at least
one <em>ill-defined concept</em> (“probability”) and one tricky concept (“specified
statistical model”). To understand what a <em>p</em> value really is, we’re going to
have to unpack both of these ideas. Along the way, we’re going to learn about
some other concepts that will also help us understand <em>Frequentist</em> inference.
And a good grounding in Frequentist inference will also help us understand the
distinction between Frequentist inference and Bayesian inference.</p>
<div id="probability" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Probability</h2>
<p>Most people think of <em>probability</em> as a mathematical concept. In a sense it is,
but it is also a deeply <em>philosophical</em> concept. We deploy the word
<em>probability</em> in many different <em>kinds</em> of situations, and it’s not clear
whether we mean the same thing in each of them. Some examples of where we use
the word probability are when we ask questions like: What is the probability of
getting heads on repeated tosses of a fair coin? What is the probability that
it will rain tomorrow? What is the probability the accused committed the crime?
The word <em>probability</em> seemingly refers to different things in each of these
situations.</p>
<p>For example, we might suggest that the probability of the getting heads is 0.5,
where this 0.5 refers to the <em>long-run relative frequency</em> of getting heads.
That is, if we were to toss a coin many many times then on around 0.5 (i.e.,
half) of the tosses the coin would come up heads.</p>
<p>We might use a different notion when thinking about the case of somebody
accused of a crime. We might say something like, “we are 90% sure” (probability
of .9) that the criminal committed the crime. But what does “90% sure” mean.
Does it make sense to think of it as the <em>relative frequency</em>? If not, then how
else might we think of it? We might, for example, think of it as a <em>credence</em>
or a <em>degree of belief</em> that the proposition is true. Or we might think of it
as a <em>degree of support</em>. That is, we might say that the available evidence
supports the hypothesis that the accused committed the crime with odds of
9-to-1.</p>
<p>This list isn’t meant to be exhaustive. The aim is just to highlight that we
might sometimes mean different things when we think about probability. It pays
to keep this in mind as we move through the course.</p>
</div>
<div id="probability-and-p-values" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Probability and <em>p</em> values</h2>
<p>Now that we know that <em>probability</em> can mean different things in different
situations, what notion of <em>probability</em> is at play in ASA’s definition of the
<em>p</em> value? The common view is to say that it refers to <em>relative frequencies</em>.
But relative frequencies of <strong>what</strong> over repeats of <strong>what</strong>?</p>
<p>We could possibly re-phrase that definition to say something like this:</p>
<blockquote>
<p>the p value refers to the relative frequency of obtaining a statistical
summary of the data as large or larger than the observed value over
hypothetical repeats of an experiment described by a specified statistical
model</p>
</blockquote>
<div id="understanding-the-p-through-simulation" class="section level3" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> Understanding the <em>p</em> through simulation</h3>
<p>One method that I think is useful for understanding statistical concept is
<em>simulation</em>. This is particularly true in the case of <em>p</em>-values, because I
definition above refers to <em>hypothetical repeats</em>. Simulation means that we can
just simulate those <em>repeats</em>. To understand how <em>p</em> values work, let’s start
with a little scenario:</p>
<blockquote>
<p>You’ve been given a device that can be used to find buried treasure. The
device has a numbered dial on it, and there is a little arrow that can point
at these numbers. The indicator never stays still, but swings around a bit.
You don’t know how the device works, except that it behaves <em>differently</em>
around treasure compared with when there is no treasure present. How can you
use this device to find treasure?</p>
</blockquote>
<p>This seems like a hard problem. You know very little about the device. You
don’t know what it’s meant to do when it finds treasure, and you don’t know
what it’s meant to do when there isn’t any treasure. So how do you go about
using it to find treasure?</p>
<div id="finding-treasure" class="section level4" number="1.2.1.1">
<h4><span class="header-section-number">1.2.1.1</span> Finding treasure</h4>
<p>The first step in using the device is to get a good description of what it does
when there isn’t any treasure around. To do this, you might just take your
device somewhere without treasure. You can then just sit and watch the dial.
After a long time watching it, you might notice that although the pointer
swings around a lot, <em>on average</em> it points at zero. This one bit of
information is enough to develop a treasure hunting strategy using this device.</p>
<p>The first step in the strategy is deciding how many readings to take on each
hunt. Because the pointer swings around a lot, we’ll need to take a couple of
readings and then use these to work out an average (which we’ll call
<span class="math inline">\(\bar{x}\)</span>). We’re in a hurry so we’ll take <strong>10</strong> readings on each hunt.</p>
<p>Next, we’ll need to scale our average. If our average is 1, then is this close
to 0? How about 0.5? Or 5? Or 15? It’s impossible to know because you don’t
know range of the average dial’s swings. So your scaling factor should be
proportional to the magnitude of the average deviations you’ve observed (we’ll
call this scaling factor <span class="math inline">\(s_{\bar{x}}\)</span>).</p>
<p>With this information in hand, we have enough to build a statistical model of
our device’s behaviour. To do this, we just go where there is no treasure and
perform the following steps: 1) Take 10 readings; 2) work out an average
(<span class="math inline">\(\bar{x}\)</span>); 3) scale it by our scaling factor (<span class="math inline">\(s_{\bar{x}}\)</span>); write down our
scaled measurement (which we’ll call <span class="math inline">\(t\)</span>), and repeat! Once we’ve done this
many many times, then we’ll have a nice distribution or statistical model of
how our device behaves when there isn’t any treasure. Of course, we don’t have
to do this for real. We can just simulate it! Feel free to play around with the
simulation, to change the numbers, and to see how this influences our
statistical model.</p>
<p><img src="01-pvalues_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>In panel <strong>A</strong> we can see the distribution of the raw averges from our device.
In panel <strong>B</strong> the averages have been scaled. Finally, in panel <strong>C</strong>, the
histogram has been turned into a density plot. The blue line shows our scaled
averages, and the dashed black line shows a <em>t</em> distribution. As we increase
the number of experiments we simulate then the two lines should begin to
overlap.</p>
</div>
<div id="using-our-device" class="section level4" number="1.2.1.2">
<h4><span class="header-section-number">1.2.1.2</span> Using our device</h4>
<p>We can use our statistical model of our device (our distribution of <em>t</em> values)
to come up with a method for finding treasure. Our statistical model tells us
what readings we’ll see when <strong>we haven’t found treasure</strong> and <strong>how often</strong>
we’ll see those readings. In the absence of treasure we’ll see readings near
the middle of the distribution very often and readings near the tails of the
distribution less often. We might even say that, in the absence of treasure, it
would be pretty <strong>surprising</strong> to see an extreme reading. Now we don’t know
anything about how the device behaves when it’s around treasure, but we know
what readings would be <em>surprising</em> if it <strong>wasn’t</strong> around treasure.</p>
<p>We can use this fact to come up with a treasure hunting rule. When you see a
<strong>surprising</strong> reading (that is, if our average of multiple readings, from a
single hunt falls in the extreme tails) the we dig for treasure. When you see
an <strong>unsurprising</strong> reading, move on to the next spot. Let’s try it out!</p>
<p>Our 10 measurements are:
0.25; -0.54; -0.51; -0.75; 0.27; -1.62; -1.29; -0.21; 0.17; -0.83</p>
<p>Our <span class="math inline">\(\bar{x}\)</span> = -0.507</p>
<p>Our <span class="math inline">\(s_{\bar{x}}\)</span> = 0.204</p>
<p>This means that our scaled measurement, <span class="math inline">\(t\)</span> = -2.485</p>
<p>Once we have our scaled reading <span class="math inline">\(t\)</span>, we can ask how <strong>surprising</strong> it is. To do
this, we just compare it against the distribution of measurements that we
generated when we weren’t around treasure.</p>
<p>96.7% of values from our simulation where closer to zero than our current value.
Only 3.3% of values where further from zero than our current value.</p>
<p>Once we have a measurement of how surprising our value is, then we just need to
set a threshold for when it’s surprising enough to warrant digging. We’ll call
this threshold <span class="math inline">\(\alpha\)</span>, and we’ll set it to 5% (for literally no reason in
particular).</p>
<p>Now let’s try using the rule. We’ll do another simulation. We’ll simulate many
many hunts, and on each hunt there either will be treasure or there won’t be
treasure. Treasure will occur with the probability of P(treasure).
We won’t know this value, because we’ll just randomly set it. For each hunt,
we’ll note down whether the rule told us to dig or move one. And we’ll also
record the ground truth to test the accuracy.</p>
<p>To asses the usefulness of our rule, we can evaluate the accuracy of our rule.
There are a few ways to do this. We can look at overall accuracy. We can look
at how often we missed treasure when there was treasure. We can look how often
we dug for treasure when there wasn’t any. Let’s take a look at some metrics.</p>
<p>The rule seems to work pretty well in terms of accuracy. But how much is
accuracy dependent on the actual probability of finding treasure? Let’s run two
more quick simulations where we set the probability of treasure actually being
present to 1 (treasure all the time) or 0 (treasure none of the time).</p>
<p>But maybe just looking at accuracy isn’t the best. After all, there are two
ways in which we can be wrong. We can dig when we’re not meant to, and we can
move on when there’s actually treasure. So let’s split that accuracy percentage
(or rather the <span class="math display">\[1 - accuracy\]</span> or “error” percentage) into two: 1) Digging
when there’s no treasure, and 2) moving on without digging when there was
treasure. Now let’s adjust P(treasure) and see how the two error
rates fare.</p>
<p>When there was no treasure at all then the <strong>false alarm
rate</strong> was 5.4 and the <strong>miss rate</strong> was
0</p>
<p>When there was treasure everywhere then the <strong>false alarm rate</strong> was
0 and the <strong>miss rate</strong> was
54.5</p>
<p>We can see that no matter what we do, the false alarm rate (digging when there
is no treasure) never goes above ~5%, which is the same value we set for
<span class="math inline">\(\alpha\)</span>. This is great because it means that we can with certainty set the
upper bound of this error rate. And, we can do so knowing nothing about how
much treasure there is to be found or how our device works in the presence of
treasure. All we need is: 1) to know that <em>on average</em> the device points at
zero when there’s no treasure around and 2) to sit and watch the device for a
long time and just record some scaled measurements that the device produces. In
fact, we don’t even need to do (2). We can just <em>pretend</em> to this by simulating
the results, and we only need to input <strong>one parameter</strong>—the same value we
that we needed for step 1. Everything else can just be made up.</p>
<p>I’m not going to talk much about the other error rate, because this isn’t a
course of frequentist inference. But we can estimate it based on some
assumptions about how the device behaves <em>in the presence of treasure</em>. For
example, if we know that treasure of a certain value results in the device
pointing on average at 1, then we can calculate the <strong>upper bound</strong> of missing
treasures smaller than that value. Trying to estimate the <strong>upper bound</strong> on
this error rate is what you’re doing when you’re doing a <strong>power analysis</strong>.
It’s generally <strong>a lot</strong> harder to estimate this, because it involves a lot of
guesswork. In comparison, estimating the first error rate is rather trivial.</p>
</div>
</div>
</div>
<div id="summary" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Summary</h2>
<p>What this rather long-winded demonstration was meant to show is that <em>p</em> values
are very good at doing one thing. That thing is, controlling how often, in the
long run, we’ll make a particular kind of error. Deployed in the right context,
they’re very good at this. This all comes from a simple process: Setting the
value of <strong>one parameter</strong>, running pretend experiments, and then comparing our
data at hand to results obtained from our pretend experiments to <strong>judge
whether our data is surprising</strong> or not.</p>
<p>Of course, our treasure-hunting scenario may not be exactly analogous to how
science works. These means that deciding whether <em>p</em> values are useful or not
is going to depend on how closely their real-world use case matches their ideal
operating environment.</p>
<div id="a-short-note-on-confidence-intervals" class="section level3" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> A short note on confidence intervals</h3>
<p>I’ll mention confidence intervals only briefly, but they follow the <em>exact</em>
same logic as <em>p</em>-values. Let’s say I collect some measurements, work out the
average. I could scale this value with my scaling factor, could get a <em>t</em>
value. I could then turn to my list of results from the pretend experiments
(the sampling distribution) to work out my <em>p</em> value. However, I can also the
sampling distribution to construct (the very poorly named) confidence interval.</p>
<p>How could we do this? Looking at the sampling distribution we constructed
earlier we would see that values that are more than about 2.23 <em>t</em> units from 0
would be surprising. Using this information, I can ask myself the following
question: If my device on average pointed at the current sample average, rather
than zero, what data values would be surprising? The answer to this is, of
course, values that are more than 2.23 <em>t</em> units from the sample mean. Having
an answer in <em>t</em> units isn’t very useful. But I know that I converted
measurements to <em>t</em> units by scaling readings using the scaling factor
<span class="math inline">\(s_{\bar{x}}\)</span>. This means we can just un-scale the value in <em>t</em> units back into
raw units using the scaling factor calculated from my sample. This means I can
say that any values <span class="math inline">\(\pm 2.23\cdot{}s_{\bar{x}}\)</span> from the sample mean would be
surprising. Any values less than this, or in this range, would be unsurprising
if my device, on average, pointed at my current sample mean. I could draw a
line through these values, put little tails on this line, and <em>hey presto</em> I
have a confidence interval.</p>
<p>Now just to be clear, just like with a <em>p</em> value, a confidence interval tells
you what values would be surprising/unsurprising on an assumption of a certain
value of the parameter. For the <em>p</em> value you set the parameter to 0 (or
wherever else the device points when no treasure is around). For the confidence
interval, I just set the parameter value to the estimate obtained from the
current sample, but it’s exactly the same idea.</p>
<p>Hopefully, this should make it clear what the confidence interval does and
doesn’t tell you. It doesn’t tell you about the <em>probability of a parameter
falling within a range</em> (the common misinterpretation). It tells you <em>frequency
with which data from pretend experiments will fall within a particular range on
the assumption that the parameter is equal to the observed value</em>. At no point
are we making inferences about <strong>parameters</strong> or <strong>true values</strong> of parameters.
We are holding parameters constant, doing pretend experiments, and then marking
out the range of surprising and unsurprising data.</p>
<p>Now that we’re all on the same page about <em>p</em> values and confidence intervals,
and we have a good idea of where they come from let us the a look at some
criticisms of <em>p</em> values.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="criticisms-of-p-values.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["bayes_notes.pdf", "pdf"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

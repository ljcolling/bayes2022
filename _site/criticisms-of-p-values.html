<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Criticisms of p values | Bayes rule</title>
  <meta name="description" content="An introduction to Bayesian Statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Criticisms of p values | Bayes rule" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introduction to Bayesian Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Criticisms of p values | Bayes rule" />
  
  <meta name="twitter:description" content="An introduction to Bayesian Statistics" />
  

<meta name="author" content="Dr Lincoln Colling" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="null-hypothesis-significance-testing.html"/>
<link rel="next" href="an-alternative-to-p-values.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Stats</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>1</b> Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="1.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability-and-p-values"><i class="fa fa-check"></i><b>1.2</b> Probability and <em>p</em> values</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#understanding-the-p-through-simulation"><i class="fa fa-check"></i><b>1.2.1</b> Understanding the <em>p</em> through simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#a-short-note-on-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> A short note on confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html"><i class="fa fa-check"></i><b>2</b> Criticisms of <em>p</em> values</a>
<ul>
<li class="chapter" data-level="2.1" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#same-measurements-from-different-devices"><i class="fa fa-check"></i><b>2.1</b> Same measurements from different devices</a></li>
<li class="chapter" data-level="2.2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#the-universe-of-possible-events"><i class="fa fa-check"></i><b>2.2</b> The universe of possible events</a></li>
<li class="chapter" data-level="2.3" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html"><i class="fa fa-check"></i><b>3</b> An alternative to <em>p</em> values</a>
<ul>
<li class="chapter" data-level="3.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#doing-inference-with-likelihoods"><i class="fa fa-check"></i><b>3.1</b> Doing inference with likelihoods</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-brief-detour-back-to-sampling-rules"><i class="fa fa-check"></i><b>3.1.1</b> A brief detour back to sampling rules</a></li>
<li class="chapter" data-level="3.1.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#the-likelihood-ratio"><i class="fa fa-check"></i><b>3.1.2</b> The likelihood ratio</a></li>
<li class="chapter" data-level="3.1.3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-note-about-likelihood-functions-and-probability-distributions"><i class="fa fa-check"></i><b>3.1.3</b> A note about likelihood functions and probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#testing-more-complex-hypotheses"><i class="fa fa-check"></i><b>3.2</b> Testing more complex hypotheses</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#theres-more-than-one-way-to-average"><i class="fa fa-check"></i><b>3.2.1</b> There’s more than one way to average</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html"><i class="fa fa-check"></i><b>4</b> The Bayes factor</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay"><i class="fa fa-check"></i><b>4.1</b> Computing Bayes factors with <code>bayesplay</code></a></li>
<li class="chapter" data-level="4.2" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay-web"><i class="fa fa-check"></i><b>4.2</b> Computing Bayes factors with Bayesplay-Web</a></li>
<li class="chapter" data-level="4.3" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#moving-beyond-coin-flips"><i class="fa fa-check"></i><b>4.3</b> Moving beyond coin flips</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="./">Lincoln Colling</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Bayes rule</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="criticisms-of-p-values" class="section level1" number="2">
<h1><span class="header-section-number">Chapter 2</span> Criticisms of <em>p</em> values</h1>
<p>People have written lots of criticisms of <em>p</em>-values. A lot of these are of the
form “<em>p</em>-values are bad because they don’t do X”, where X is not a design
feature of frequentist inference. I’m not interested in these kinds of
criticisms, because they seem pretty meaningless. Instead, I think that if we
are going to criticise <em>p</em>-values it is better to look at the design features
of frequentist inference and find fault there.</p>
<p>So what are the design features? In the last section, we saw how frequentist
inference was very good at controlling the kinds of mistakes we made in our
treasure hunt. To do this, all we needed was a model of how our treasure
detecting device operated. If we only wanted to control <em>false positives</em> all
we needed was a model of how it operated in the absence of treasure—we didn’t
even need to know how it behaved when there was treasure around! To build this
model we needed one bit of information—that the dial <em>on average</em> pointed at 0
when there was no treasure. The whole model could then be built up by running
lots of simulations (or pretend experiments) where this parameter (the average
reading in the absence of treasure) was the only parameter we needed to set.
Just doing this allows us to precisely set an upper-bound on how often we make
false positives. That’s a pretty powerful property, and it all comes from such
a simple principle.</p>
<p>But are there some issues with this simple principle? We can try explore it a
bit more and see where things start to break.</p>
<div id="same-measurements-from-different-devices" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Same measurements from different devices</h2>
<p>Let’s imagine a new scenario. As before, you have a treasure hunting device
(we’ll call it <span class="math inline">\(d_1\)</span>). You’re using <span class="math inline">\(d_1\)</span> to hunt for treasure, and using the
readings to decide whether to dig or not. At your first treasure hunting spot,
you record the measurements: 1, 0, 1, 3, 0, 1, 4, -1, 3, 4. You then average,
and scale these measurements and get a <em>t</em> value of approximately 2.848. You
compare this to what you found in your imaginary experiments and find <em>p</em> =
.019. According to your rule, that means you dig. For far so good.</p>
<p>However, before you start digging, I run up to you and tell you that device
<span class="math inline">\(d_1\)</span> is broken. I tested it before you left, and found that <span class="math inline">\(d_1\)</span> is incapable
of measuring values bigger than 6. You look at your measurements again, and to
your relief, they don’t go anywhere near 6. Your highest measurement is only 4.
But should you worry that the device couldn’t register values of 6 or higher?
And if so, why?</p>
<p>More generally, how would this fault with the device influence your treasure
hunting strategy and would it change your view of when you think you should
start digging? The intuition here might be a little unclear, so let’s modify
the example a little bit.</p>
<p>In the modified example, you want to be extra careful to avoid taking a broken
device with you, so you take two measurement devices (<span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>). The
devices are identical and, indeed, when you look at the measurements you can
see that they’ve recorded an identical set of 10 numbers. Because the
measurements are the same, you just pick whichever device and work out your
scaled reading and decide whether to dig.</p>
<p>But not so fast, I again tell you that <span class="math inline">\(d_1\)</span> is actually broken and it is
incapable of recording measurements higher than 6. I also tell you <span class="math inline">\(d_2\)</span> is
working perfectly. What does this do to your inference? Does your inference
change depending on whether you decided to look at <span class="math inline">\(d_1\)</span> or <span class="math inline">\(d_2\)</span>? Remember,
that the actual numbers produced by both machines are identical.</p>
<p>If you want to be a good <em>frequentist</em> then the answer to this question is a
resounding <em>yes</em>. Even though <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> produced the exact same
measurements, and despite these measurements being accurate, your inference
will depend on the device you decided to look at. But why? Understanding the
answer to this means going back to the sampling distribution we generated by
running pretend experiments. Let’s run some new pretend experiments for <span class="math inline">\(d_1\)</span>
and <span class="math inline">\(d_2\)</span>. The stimulations for <span class="math inline">\(d_1\)</span> will be modified slightly so that all
values higher than 6 will be replaced with a 6.</p>
<p><img src="02-problems_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>As you can see, the distributions are different. This is because in those
pretend experiments, the devices would behave differently. In our actual
experiment (this treasure hunt), they didn’t behave differently. They behaved
exactly the same, and both behaved accurately. Remember, these distributions
are what we use to make a judgement about whether our reading is surprising or
not. We mark out sections of these distributions to find the range of values
that are surprising and the range of values that are unsurprising. Because the
shape of these distributions are different, the ranges that we mark out on each
of them will be different. And consequently what counts as a
surprising/unsurprising value on one distribution might not count as a
surprising/unsurprising value on the other one.</p>
<p>If you’re being a frequentist then there’s no getting away from the fact that
because the devices have the <em>potential</em> to behave differently in situations
other than the current situation, this <em>potential difference</em> must be accounted
for. They factor into the calculation of the <em>p</em> value by changing the
distributions and, therefore, we need to take account of these potential events
in our inferences if we want to maintain our error control properties.</p>
<p>For some, the influence of imaginary events is madness. Jeffreys described this
“madness” as follows:</p>
<blockquote>
<p>What the use of P implies, therefore, is that a hypothesis that may be true
may be rejected because it has not predicted observable results that have not
occurred. This seems a remarkable procedure (Jeffreys, 1961, p. 385)</p>
</blockquote>
</div>
<div id="the-universe-of-possible-events" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> The universe of possible events</h2>
<p>To see another example of how potential events can influence inferences, let us
examine a different scenario. In this scenario, we’re going to make judgements
about the fairness of a coin (fair coins being defined as coins that show heads
with P(heads) = 0.5). We’ll use the same procedure as our treasure hunting
device. We will flip a coin that we <em>know</em> is fair a set number of times (let’s
say 10 times). We then count up <strong>x</strong> heads out of our total of <strong>n</strong> flips. We
then repeat the procedure many many times. We can use this procedure to
generate a distribution of possible data. Again, we can just simulate this.</p>
<p><img src="02-problems_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Armed with this distribution, we can start making judgements about actual data.
To produce some real data, I’ll flip the coin I want to test and, at the end,
I’ll count up the number of heads. Let’s say that I got 8 heads and 2 tails.
Now you can make a judgement about whether this data is surprising or not. To
do this, all you need to do is compare it to the simulated results above.</p>
<p>The <em>p</em> value for 8 heads in 10 flips is 0.109.</p>
<p>This result is not surprising
on the assumption that the coin is fair (i.e., P(heads) =
0.5)</p>
<p>But save your judgement for now, because there’s something that I have
neglected to tell you. My plan wasn’t to flip the coin 10 times. Instead, I
decided that I would just flip the coin until it came up tails twice, and it
just so happened that on this occasion this meant that I flipped the coin 10
times.</p>
<p>Does this fact change your inference? If our inferences are based on comparing
our actual data to possible data then we need to examine whether this sampling
rule changes the possible data that could have been generated. That is, we need
to take into account whether the data was generated by deciding to flip the
coin 10 times or whether it just so happened that I flipped the coin 10 times,
but really “in my head” I was going to stop when I got 2 tails. To so see why
we need to re-run the simulations. In the new simulations for each sample we’ll
continue to flip the coin until it comes up with 2 heads, and then we’ll stop.
Sometimes this will mean that the coin is flipped 10 times, but sometimes we
might flip it more, and sometimes we might flip it less.</p>
<p>We now can count up the relative frequency of getting 2 heads after 2 flips,
after 3 flips, 4 flips, and so on. And we can draw a plot of this distribution.</p>
<p><img src="02-problems_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>From this new distribution, we can now ask: How often would you need to flip a
fair coin 10 or more times before you got two heads? That is, is it surprising
that we had to flip it this many times? Let’s see how the inference differs.</p>
<p>For a fair coin (P(heads) = 0.5),
about 98% of experiments would end before we got to
10 flips. Only 2% of experiments would run this
long. Therefore, our result is surprising!</p>
</div>
<div id="summary-1" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Summary</h2>
<p>What these two examples (the broken device, and stopping rule example) show is
that even when presented with the <strong>same data</strong> the inferences we make about
that data will be different if the realm of <strong>possible</strong>, but <strong>not actual</strong>
results are different. That is, non-existent results influence our inferences.
A broken device that still behaved accurately when we used it influences our
inferences, and what we had going on inside our head when we collected our data
also made a difference. Based on this, we can go ahead to imagine even more
ridiculous examples.</p>
<p>For example, imagine that I build a device that is going to flip a coin to
decide whether 1) to flip the coin n times or 2) flip it until it comes up
tails x times. The device makes a decision, flips the coin, and it just so
happens that on this occasion we get 8 heads and 2 tails. How do I analyse this
set of data? Does the realm of possible data include the machine that makes the
decision? What if I know what decision the device made? Do I still have to take
into account the experiment that wasn’t performed? And what if I have the
results of two experiments, one that was performed as part of a mixture (using
a machine to decide which of the two experiments would be performed) and one
that was not performed as part of a mixture. If they yield the same data, then
does the fact that one was part of a mixture mean that the conclusions should
be different? For a frequentist, these can be pretty uncomfortable questions!
In the next section we’re going to see if we can find a way out of this bind.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="null-hypothesis-significance-testing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="an-alternative-to-p-values.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["bayes_notes.pdf", "pdf"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

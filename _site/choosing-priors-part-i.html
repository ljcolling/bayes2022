<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Choosing priors: Part I | Advanced Statistical Methods</title>
  <meta name="description" content="An introduction to Bayesian Statistics" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Choosing priors: Part I | Advanced Statistical Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An introduction to Bayesian Statistics" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Choosing priors: Part I | Advanced Statistical Methods" />
  
  <meta name="twitter:description" content="An introduction to Bayesian Statistics" />
  

<meta name="author" content="Dr Lincoln Colling" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moving-beyond-coin-flips-1.html"/>
<link rel="next" href="choosing-priors-part-ii.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Bayesian Stats</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#how-to-use-this-book"><i class="fa fa-check"></i>How to use this book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#outline"><i class="fa fa-check"></i>Outline</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>1</b> Null-hypothesis significance testing</a>
<ul>
<li class="chapter" data-level="1.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability"><i class="fa fa-check"></i><b>1.1</b> Probability</a></li>
<li class="chapter" data-level="1.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#probability-and-p-values"><i class="fa fa-check"></i><b>1.2</b> Probability and <em>p</em> values</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#understanding-the-p-through-simulation"><i class="fa fa-check"></i><b>1.2.1</b> Understanding the <em>p</em> through simulation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#summary"><i class="fa fa-check"></i><b>1.3</b> Summary</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#a-short-note-on-confidence-intervals"><i class="fa fa-check"></i><b>1.3.1</b> A short note on confidence intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html"><i class="fa fa-check"></i><b>2</b> Criticisms of <em>p</em> values</a>
<ul>
<li class="chapter" data-level="2.1" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#same-measurements-from-different-devices"><i class="fa fa-check"></i><b>2.1</b> Same measurements from different devices</a></li>
<li class="chapter" data-level="2.2" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#the-universe-of-possible-events"><i class="fa fa-check"></i><b>2.2</b> The universe of possible events</a></li>
<li class="chapter" data-level="2.3" data-path="criticisms-of-p-values.html"><a href="criticisms-of-p-values.html#summary-1"><i class="fa fa-check"></i><b>2.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html"><i class="fa fa-check"></i><b>3</b> An alternative to <em>p</em> values</a>
<ul>
<li class="chapter" data-level="3.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#doing-inference-with-likelihoods"><i class="fa fa-check"></i><b>3.1</b> Doing inference with likelihoods</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-brief-detour-back-to-sampling-rules"><i class="fa fa-check"></i><b>3.1.1</b> A brief detour back to sampling rules</a></li>
<li class="chapter" data-level="3.1.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#the-likelihood-ratio"><i class="fa fa-check"></i><b>3.1.2</b> The likelihood ratio</a></li>
<li class="chapter" data-level="3.1.3" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#a-note-about-likelihood-functions-and-probability-distributions"><i class="fa fa-check"></i><b>3.1.3</b> A note about likelihood functions and probability distributions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#testing-more-complex-hypotheses"><i class="fa fa-check"></i><b>3.2</b> Testing more complex hypotheses</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="an-alternative-to-p-values.html"><a href="an-alternative-to-p-values.html#theres-more-than-one-way-to-average"><i class="fa fa-check"></i><b>3.2.1</b> There’s more than one way to average</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html"><i class="fa fa-check"></i><b>4</b> The Bayes factor</a>
<ul>
<li class="chapter" data-level="4.1" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay"><i class="fa fa-check"></i><b>4.1</b> Computing Bayes factors with <code>bayesplay</code></a></li>
<li class="chapter" data-level="4.2" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#computing-bayes-factors-with-bayesplay-web"><i class="fa fa-check"></i><b>4.2</b> Computing Bayes factors with Bayesplay-Web</a></li>
<li class="chapter" data-level="4.3" data-path="the-bayes-factor.html"><a href="the-bayes-factor.html#moving-beyond-coin-flips"><i class="fa fa-check"></i><b>4.3</b> Moving beyond coin flips</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html"><i class="fa fa-check"></i><b>5</b> Moving beyond coin flips</a>
<ul>
<li class="chapter" data-level="5.1" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html#choosing-a-likelihood"><i class="fa fa-check"></i><b>5.1</b> Choosing a likelihood</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html#the-variance-of-likelihoods"><i class="fa fa-check"></i><b>5.1.1</b> The variance of likelihoods</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html#inferences-about-raw-means"><i class="fa fa-check"></i><b>5.2</b> Inferences about raw means</a></li>
<li class="chapter" data-level="5.3" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html#inferences-about-effect-sizes"><i class="fa fa-check"></i><b>5.3</b> Inferences about effect sizes</a></li>
<li class="chapter" data-level="5.4" data-path="moving-beyond-coin-flips-1.html"><a href="moving-beyond-coin-flips-1.html#inferences-about-t-values"><i class="fa fa-check"></i><b>5.4</b> Inferences about <em>t</em> values</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="choosing-priors-part-i.html"><a href="choosing-priors-part-i.html"><i class="fa fa-check"></i><b>6</b> Choosing priors: Part I</a>
<ul>
<li class="chapter" data-level="6.1" data-path="choosing-priors-part-i.html"><a href="choosing-priors-part-i.html#reference-objective-uninformative-and-default-priors"><i class="fa fa-check"></i><b>6.1</b> Reference, objective, uninformative, and default priors</a></li>
<li class="chapter" data-level="6.2" data-path="choosing-priors-part-i.html"><a href="choosing-priors-part-i.html#default-priors-for-effect-sizes"><i class="fa fa-check"></i><b>6.2</b> Default priors for effect sizes</a></li>
<li class="chapter" data-level="6.3" data-path="choosing-priors-part-i.html"><a href="choosing-priors-part-i.html#interpreting-our-bayes-factors"><i class="fa fa-check"></i><b>6.3</b> Interpreting our Bayes factors</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html"><i class="fa fa-check"></i><b>7</b> Choosing priors: Part II</a>
<ul>
<li class="chapter" data-level="7.1" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html#examples-of-representing-predictions"><i class="fa fa-check"></i><b>7.1</b> Examples of representing predictions</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html#representing-predictions-with-a-uniform-prior"><i class="fa fa-check"></i><b>7.1.1</b> Representing predictions with a uniform prior</a></li>
<li class="chapter" data-level="7.1.2" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html#representing-predictions-with-a-half-normal-prior"><i class="fa fa-check"></i><b>7.1.2</b> Representing predictions with a half-normal prior</a></li>
<li class="chapter" data-level="7.1.3" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html#representing-predictions-with-the-normal-prior"><i class="fa fa-check"></i><b>7.1.3</b> Representing predictions with the normal prior</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="choosing-priors-part-ii.html"><a href="choosing-priors-part-ii.html#how-do-i-know-what-my-theory-predicts"><i class="fa fa-check"></i><b>7.2</b> How do I know what my theory predicts</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="./">Lincoln Colling</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Advanced Statistical Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="choosing-priors-part-i" class="section level1" number="6">
<h1><span class="header-section-number">Chapter 6</span> Choosing priors: Part I</h1>
<p><a href="data:text/x-markdown;base64,---
title: "Priors: Part 1"
output: html_document
---

```{r setup, echo=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
suppressMessages(expr = {
  if ("xfun" %in% row.names(installed.packages()) == FALSE) {
    install.packages("xfun")
  }

  display_markdown <<- knitr::asis_output
  display_html <<- knitr::asis_output

  xfun::pkg_attach(
    c(
      "tidyverse",
      "polspline",
      "patchwork",
      "magrittr",
      "bayesplay",
      "knitr",
      "broom",
      "bayesplay"
    ),
    install = TRUE
  )
})

table_format <- "html"
```
# Choosing priors: Part I

In the previous section, we learned how we can use normal, student *t*,
and various kinds of *non-central t* likelihoods to model means, mean
differences, and effect sizes. But if we actually want to compute Bayes
factors then we'll also need to define **priors**. While the
**likelihoods** are a model of our **data** the **priors** will serve as
the models for the **hypotheses** we actually want to compare.

There are two broad schools of thought when it comes to defining priors.
The first is to choose priors that can be used in a wide range of
situations and don't need to be tailored to the specifics of the situation
at hand. This is often framed in terms of selecting priors that aren't
dependent on the individual beliefs or theories of a specific researcher,
or priors that represent ignorance about any possible effect. These are
sometimes called objective, reference, uninformative, or default priors
(these terms aren't exactly synonymous, but for our purposes the technical
differences won't matter). 

The second approach is to choose priors that are specific to the situation
at hand. This might be by selecting priors that represent actual
scientific theories, selecting priors that constrain the predicted effects
to be within the expected range, or choosing priors based on, for example,
previous evidence about the nature of the effect being studied. These
kinds of priors go under the label of informed, or subjective priors. It
is also important to note that the lines between the two approaches is not
always clear cut. Rather, they are often blurred.

## Reference, objective, uninformative, and default priors

The most straightforward way to come up with a prior that can work in
a wide range of situations is to use the *principle of indifference*. This
is the approach that we used when we were coming up with our very first
prior for the coin flip example. Our reasoning was roughly as follows:

- If we don't know what the coin bias is (just that it is some value
  between 0 and 1), then we have no reason for predicting that any
  particular outcome (i.e., number of heads after a particular number of
  flips) will occur more often than any other particular outcome.

- If we flip the coin $n$ times, then there are $n + 1$ possible
  outcomes. Therefore, we assign a probability of $\frac{1}{n+1}$ to each
  outcome. 

- The prior that fits with this prediction is a uniform prior from 0 to 1.

The idea here is that in coming up with the prior we're trying to make as
few assumptions as possible. Coming up with priors that make as few
assumptions as possible is not always straightforward. There are a number
of technical difficulties that can arise when choosing priors that
*seemingly* don't make any assumptions. Some of these issues arise when,
for example, choosing a prior that is non-informative when a question is
asked one way (for example, asking about the *bias* of the coin) but then
doesn't turn out to be non-informative when the question is asked in
a different, but equivalent way (for example, asking about the *log odds*
of obtaining heads). 

Because of these technical difficulties, people have come up with rules
for choosing priors that make as few assumptions as possible. Once such
rule if Jeffrey's rule. A detailed treatment is Jeffrey's rule is outside
the scope of this course, but it is interesting to note that Jeffrey's
rule relies on the *realm of possible events* (the same thing that caused
our worries about *p*-values being impacted by different sampling rules). 

## Default priors for effect sizes

Another method for defining **objective** priors that has been
particularly popular within psychology has been to use **default priors**.
The most prominent example of this approach has been the use of **default
priors** for effect sizes---the so-called *default Bayesian t-test*
[(Rouder et al,
2009)](http://pcl.missouri.edu/sites/default/files/Rouder.bf_.pdf).

The *default Bayesian t-test* can be used anywhere where a regular
frequentist *t*-test can be used. For the default Bayes *t*-test, the data
are modelled in terms of the effect size. That is, a *non-central d* or
*non-central d2* likelihood is used (depending on whether the data are
from one-sample/paired data or independent samples). These are the
likelihood's that we defined near the end of the previous section.
However, what really characterises this approach is the prior that is
employed. The default Bayes *t*-test uses a **Cauchy** prior. A **Cauchy**
distribution is similar in shape to a standard normal distribution (panel
A below), however it has far fatter tails (panel B below).

```{r}
normal <- prior("normal", 0, 1)
cauchy <- prior("cauchy", 0, 1)
((plot(normal) +
  theme_minimal(14) +
  NULL) /
  (plot(cauchy) +
    theme_minimal(14) +
    NULL) +
  plot_annotation(tag_levels = "A"))
```

For a more in-depth discussion of Cauchy priors, a recent paper by
[Schmalz et al, 2021](https://osf.io/5geqt/download) is highly
recommended. We'll learn about them by exploring some of their properties
using `bayesplay`. As you can see from the plots above, compared to
a normal distribution, the Cauchy has far less mass in the middle of the
distribution. For the Cauchy distribution, 50% of the distribution lies
between -1 and +1 while for the normal distribution 68% of the
distribution lies between -1 and +1. 

We can define a **Cauchy** prior using the `prior` function from
`bayesplay` and setting the **family** to **cauchy**. Two other values can
also be set. The first is **location** which determines the centre of the
distribution. This has a default value of 0. The second is **scale** which
can change how wide or narrow the distribution is. The original paper by
Rouder et al (2009) set this value to 1. However, now a value of
$\frac{1}{\sqrt{2}}\approx0.707$ is more typical, and this is the default
value in their R package (called `BayesFactor`).

Let us define a Cauchy prior with a location of 0, and a scale of 1.

```{r}
#| echo = TRUE, include = TRUE

standard_cauchy <- prior(
  family = "cauchy",
  location = 0,
  scale = 1
)
```

And now we'll define a Cauchy prior with a location of 0, and a scale of 
$\frac{1}{\sqrt{2}}$.


```{r}
#| echo = TRUE, include = TRUE

medium_cauchy <- prior(
  family = "cauchy",
  location = 0,
  scale = 1 / sqrt(2)
)
```


With both priors defined we can plot them above each other.

```{r}
#| echo = TRUE, include = TRUE

standard_cauchy_plot <- plot(standard_cauchy) +
  theme_minimal(14) +
  theme(title = element_text(size = 8)) +
  labs(subtitle = "Cauchy(0, 1)")


medium_cauchy_plot <- plot(medium_cauchy) +
  theme_minimal(14) +
  theme(title = element_text(size = 8)) +
  labs(subtitle = "Cauchy(0, 0.707)")


standard_cauchy_plot / medium_cauchy_plot
```

Although the motivation behind the default Bayes *t*-test is to come up
with objective priors, Rouder et al (2009) also note that re-scaling the
prior to be wider or narrower, depending on the range of predicted effect
sizes, can be a way to *tune* the prior to the particulars of the
experiment. As mentioned earlier, the divide between **objective** and
**subjective** priors is a blurry one.

One of these Cauchy priors is going to represent our *alternative
hypotheses*, but we also need a prior to represent our *null hypothesis*.
To keep things simple we'll just use a point hypothesis at zero.

Now that we've decided on the priors we're going to use, we need to get to
the most important bit! The data. We'll analyse the data from the previous
section. We'll do both the one-sample, and the two-sample case. This means
that we can use the likelihoods that we defined in the previous section,
and we just need to add the priors.

In the first one sample case, we found a *d* of 0.23 with a sample size
of 80. For our alternative hypothesis, we'll use the narrower Cauchy
distribution. That is, a Cauchy with a location of 0 and a scale of 0.707.
And for the null hypothesis we'll use a point at 0. 

I'll use the [bayesplay web-app](https://bayesplay.mindsci.net) to define
the model. The setting's are just as follows.

First the likelihood:

![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_likelihood.png)

Then the alternative prior:

![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_alt.png)

And then the null prior:

![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_null.png)

With these values entered, I can generate the R code. This code is shown
below.


```{r}
#| include = TRUE, echo = TRUE
# define likelihood
data_model <- likelihood(family = "noncentral_d", d = 0.23, n = 80)

# define alternative prior
alt_prior <- prior(family = "cauchy", location = 0, scale = 0.707)

# define null prior
null_prior <- prior(family = "point", point = 0)

# weight likelihood by prior
m1 <- data_model * alt_prior
m0 <- data_model * null_prior

# take the intergal of each weighted likelihood
# and divide them
bf <- integral(m1) / integral(m0)
bf
```

And now we can give a bit of a description of our result.

```{r}
glue::glue("The Bayes factor is {round(bf,2)}. This means that the
data are {round(bf,2)} times more likely under our alternative hypothesis
relative to our null hypothesis.") %>%
  display_markdown()
```

With the one-sample case out of the way, we can now turn our attention to
the two sample case. We'll use the same priors as before, but now we'll
use the **noncentral_d2** likelihood that we used to model this data in
the previous section. Because we're using the same priors as before, we
can just update our likelihood from the previous chunk of code, and keep
everything else the same. For this new likelihood, we'll have a *d* of
0.99 and sample sizes of 13 and 12.

```{r}
#| include = TRUE, echo = TRUE
# define likelihood
data_model <- likelihood(
  family = "noncentral_d2",
  d = 0.99,
  n1 = 13,
  n2 = 12
)

# define alternative prior
alt_prior <- prior(family = "cauchy", location = 0, scale = 0.707)

# define null prior
null_prior <- prior(family = "point", point = 0)

# weight likelihood by prior
m1 <- data_model * alt_prior
m0 <- data_model * null_prior

# take the intergal of each weighted likelihood
# and divide them
bf <- integral(m1) / integral(m0)
bf
```

And now a description of our result.

```{r}
glue::glue("The Bayes factor is {round(bf,2)}. This means that the
data are {round(bf,2)} times more likely under our alternative hypothesis
relative to our null hypothesis.") %>%
  display_markdown()
```

## Interpreting our Bayes factors

Now that we have both Bayes factors let's think a little bit about what
they actually mean. What the Bayes factors did was compare two hypotheses.
The first hypothesis (our null hypothesis) said that the effect
size---that is, the difference in, for example, the accuracy of
remembering words in the two conditions---was 0. The second hypothesis
said that the effect size was **not** 0. But more specifically, it said
the effect size was **not** 0 in the specific way as described by the
specific Cauchy prior that we used. 

This Cauchy prior says that we think that if there is an effect, that it
is probably somewhere between about -2.2 and 2.2 (that is, 80% of the
prior distribution lies between these values. [Schmalz et al,
2021](https://osf.io/5geqt/download) provides a handy table that tells you
the 50% and 80% bounds for Cauchy priors of different scales (labelled
JASP scale factor on the table).

![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/xenia_table.png)

However, if you're comfortable with `R` then you can work it out yourself. 
For example, the code below calculates how much of the defined prior is
between -2 and 2.

```{r}
#| echo = TRUE, include = TRUE

# define the prior
p <- prior("cauchy", 0, .707)

# work out how much of it is between -2 and 2
integrate(Vectorize(p$prior_function), -2.2, 2.2)$value
```

The Bayes factor calculation sets up two hypotheses about what we think
about the effect size in the case that there is no effect (the null) and
in the case that we think there is an effect (the alternative) and tells
us under which of these two scenarios we'd be more likely to observe our
data.

Note, however, that the two hypotheses that we compared are only two out
of a possible infinite set of hypotheses. I might, for example, think that
if there is an effect then it is **not** zero in a different way. I might,
for example, think that if there is an effect then it will be greater then
0 in a specific way as described by my prior. That is, I might one to
perform a one-sided test rather than a two-sided (or two-tailed in
frequentist terms) test. 

To do this, all I would need to do is update my prior. In the web-app
I can do this by toggling the limit switches and setting the lower limit
to 0. As you can see the Cauchy prior is now cut in half so that it only
contains values greater than 0.

![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/truncate.png)

If we were to generate the `R` code, we'd see that the alternative prior
is now defined as follows:

```{r}
#| include = TRUE, echo = TRUE

# define alternative prior
alt_prior <- prior(
  family = "cauchy",
  location = 0,
  scale = 0.707,
  range = c(0, Inf)
)
```

In fact, I could test any arbitrary sets of hypotheses I want. In the next
section, on informed or subjective priors we'll see how we can compare any
arbitrary set of hypotheses we want.
" download="06-prior-I.Rmd"><img src="https://img.shields.io/badge/.Rmd-Download-blue"></a>
<a href="https://colab.research.google.com/github/ljcolling/bayes2022/blob/main/_notebooks/06-prior-I.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>
<p>In the previous section, we learned how we can use normal, student <em>t</em>,
and various kinds of <em>non-central t</em> likelihoods to model means, mean
differences, and effect sizes. But if we actually want to compute Bayes
factors then we’ll also need to define <strong>priors</strong>. While the
<strong>likelihoods</strong> are a model of our <strong>data</strong> the <strong>priors</strong> will serve as
the models for the <strong>hypotheses</strong> we actually want to compare.</p>
<p>There are two broad schools of thought when it comes to defining priors.
The first is to choose priors that can be used in a wide range of
situations and don’t need to be tailored to the specifics of the situation
at hand. This is often framed in terms of selecting priors that aren’t
dependent on the individual beliefs or theories of a specific researcher,
or priors that represent ignorance about any possible effect. These are
sometimes called objective, reference, uninformative, or default priors
(these terms aren’t exactly synonymous, but for our purposes the technical
differences won’t matter).</p>
<p>The second approach is to choose priors that are specific to the situation
at hand. This might be by selecting priors that represent actual
scientific theories, selecting priors that constrain the predicted effects
to be within the expected range, or choosing priors based on, for example,
previous evidence about the nature of the effect being studied. These
kinds of priors go under the label of informed, or subjective priors. It
is also important to note that the lines between the two approaches is not
always clear cut. Rather, they are often blurred.</p>
<div id="reference-objective-uninformative-and-default-priors" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Reference, objective, uninformative, and default priors</h2>
<p>The most straightforward way to come up with a prior that can work in
a wide range of situations is to use the <em>principle of indifference</em>. This
is the approach that we used when we were coming up with our very first
prior for the coin flip example. Our reasoning was roughly as follows:</p>
<ul>
<li><p>If we don’t know what the coin bias is (just that it is some value
between 0 and 1), then we have no reason for predicting that any
particular outcome (i.e., number of heads after a particular number of
flips) will occur more often than any other particular outcome.</p></li>
<li><p>If we flip the coin <span class="math inline">\(n\)</span> times, then there are <span class="math inline">\(n + 1\)</span> possible
outcomes. Therefore, we assign a probability of <span class="math inline">\(\frac{1}{n+1}\)</span> to each
outcome.</p></li>
<li><p>The prior that fits with this prediction is a uniform prior from 0 to 1.</p></li>
</ul>
<p>The idea here is that in coming up with the prior we’re trying to make as
few assumptions as possible. Coming up with priors that make as few
assumptions as possible is not always straightforward. There are a number
of technical difficulties that can arise when choosing priors that
<em>seemingly</em> don’t make any assumptions. Some of these issues arise when,
for example, choosing a prior that is non-informative when a question is
asked one way (for example, asking about the <em>bias</em> of the coin) but then
doesn’t turn out to be non-informative when the question is asked in
a different, but equivalent way (for example, asking about the <em>log odds</em>
of obtaining heads).</p>
<p>Because of these technical difficulties, people have come up with rules
for choosing priors that make as few assumptions as possible. Once such
rule if Jeffrey’s rule. A detailed treatment is Jeffrey’s rule is outside
the scope of this course, but it is interesting to note that Jeffrey’s
rule relies on the <em>realm of possible events</em> (the same thing that caused
our worries about <em>p</em>-values being impacted by different sampling rules).</p>
</div>
<div id="default-priors-for-effect-sizes" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Default priors for effect sizes</h2>
<p>Another method for defining <strong>objective</strong> priors that has been
particularly popular within psychology has been to use <strong>default priors</strong>.
The most prominent example of this approach has been the use of <strong>default
priors</strong> for effect sizes—the so-called <em>default Bayesian t-test</em>
<a href="http://pcl.missouri.edu/sites/default/files/Rouder.bf_.pdf">(Rouder et al,
2009)</a>.</p>
<p>The <em>default Bayesian t-test</em> can be used anywhere where a regular
frequentist <em>t</em>-test can be used. For the default Bayes <em>t</em>-test, the data
are modelled in terms of the effect size. That is, a <em>non-central d</em> or
<em>non-central d2</em> likelihood is used (depending on whether the data are
from one-sample/paired data or independent samples). These are the
likelihood’s that we defined near the end of the previous section.
However, what really characterises this approach is the prior that is
employed. The default Bayes <em>t</em>-test uses a <strong>Cauchy</strong> prior. A <strong>Cauchy</strong>
distribution is similar in shape to a standard normal distribution (panel
A below), however it has far fatter tails (panel B below).</p>
<p><img src="06-prior-I_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>For a more in-depth discussion of Cauchy priors, a recent paper by
<a href="https://osf.io/5geqt/download">Schmalz et al, 2021</a> is highly
recommended. We’ll learn about them by exploring some of their properties
using <code>bayesplay</code>. As you can see from the plots above, compared to
a normal distribution, the Cauchy has far less mass in the middle of the
distribution. For the Cauchy distribution, 50% of the distribution lies
between -1 and +1 while for the normal distribution 68% of the
distribution lies between -1 and +1.</p>
<p>We can define a <strong>Cauchy</strong> prior using the <code>prior</code> function from
<code>bayesplay</code> and setting the <strong>family</strong> to <strong>cauchy</strong>. Two other values can
also be set. The first is <strong>location</strong> which determines the centre of the
distribution. This has a default value of 0. The second is <strong>scale</strong> which
can change how wide or narrow the distribution is. The original paper by
Rouder et al (2009) set this value to 1. However, now a value of
<span class="math inline">\(\frac{1}{\sqrt{2}}\approx0.707\)</span> is more typical, and this is the default
value in their R package (called <code>BayesFactor</code>).</p>
<p>Let us define a Cauchy prior with a location of 0, and a scale of 1.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="choosing-priors-part-i.html#cb20-1" aria-hidden="true" tabindex="-1"></a>standard_cauchy <span class="ot">&lt;-</span> <span class="fu">prior</span>(</span>
<span id="cb20-2"><a href="choosing-priors-part-i.html#cb20-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;cauchy&quot;</span>,</span>
<span id="cb20-3"><a href="choosing-priors-part-i.html#cb20-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">location =</span> <span class="dv">0</span>,</span>
<span id="cb20-4"><a href="choosing-priors-part-i.html#cb20-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">scale =</span> <span class="dv">1</span></span>
<span id="cb20-5"><a href="choosing-priors-part-i.html#cb20-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>And now we’ll define a Cauchy prior with a location of 0, and a scale of
<span class="math inline">\(\frac{1}{\sqrt{2}}\)</span>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="choosing-priors-part-i.html#cb21-1" aria-hidden="true" tabindex="-1"></a>medium_cauchy <span class="ot">&lt;-</span> <span class="fu">prior</span>(</span>
<span id="cb21-2"><a href="choosing-priors-part-i.html#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;cauchy&quot;</span>,</span>
<span id="cb21-3"><a href="choosing-priors-part-i.html#cb21-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">location =</span> <span class="dv">0</span>,</span>
<span id="cb21-4"><a href="choosing-priors-part-i.html#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">scale =</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="dv">2</span>)</span>
<span id="cb21-5"><a href="choosing-priors-part-i.html#cb21-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>With both priors defined we can plot them above each other.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="choosing-priors-part-i.html#cb22-1" aria-hidden="true" tabindex="-1"></a>standard_cauchy_plot <span class="ot">&lt;-</span> <span class="fu">plot</span>(standard_cauchy) <span class="sc">+</span></span>
<span id="cb22-2"><a href="choosing-priors-part-i.html#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="dv">14</span>) <span class="sc">+</span></span>
<span id="cb22-3"><a href="choosing-priors-part-i.html#cb22-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">8</span>)) <span class="sc">+</span></span>
<span id="cb22-4"><a href="choosing-priors-part-i.html#cb22-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Cauchy(0, 1)&quot;</span>)</span>
<span id="cb22-5"><a href="choosing-priors-part-i.html#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="choosing-priors-part-i.html#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="choosing-priors-part-i.html#cb22-7" aria-hidden="true" tabindex="-1"></a>medium_cauchy_plot <span class="ot">&lt;-</span> <span class="fu">plot</span>(medium_cauchy) <span class="sc">+</span></span>
<span id="cb22-8"><a href="choosing-priors-part-i.html#cb22-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="dv">14</span>) <span class="sc">+</span></span>
<span id="cb22-9"><a href="choosing-priors-part-i.html#cb22-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">8</span>)) <span class="sc">+</span></span>
<span id="cb22-10"><a href="choosing-priors-part-i.html#cb22-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">subtitle =</span> <span class="st">&quot;Cauchy(0, 0.707)&quot;</span>)</span>
<span id="cb22-11"><a href="choosing-priors-part-i.html#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="choosing-priors-part-i.html#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="choosing-priors-part-i.html#cb22-13" aria-hidden="true" tabindex="-1"></a>standard_cauchy_plot <span class="sc">/</span> medium_cauchy_plot</span></code></pre></div>
<p><img src="06-prior-I_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Although the motivation behind the default Bayes <em>t</em>-test is to come up
with objective priors, Rouder et al (2009) also note that re-scaling the
prior to be wider or narrower, depending on the range of predicted effect
sizes, can be a way to <em>tune</em> the prior to the particulars of the
experiment. As mentioned earlier, the divide between <strong>objective</strong> and
<strong>subjective</strong> priors is a blurry one.</p>
<p>One of these Cauchy priors is going to represent our <em>alternative
hypotheses</em>, but we also need a prior to represent our <em>null hypothesis</em>.
To keep things simple we’ll just use a point hypothesis at zero.</p>
<p>Now that we’ve decided on the priors we’re going to use, we need to get to
the most important bit! The data. We’ll analyse the data from the previous
section. We’ll do both the one-sample, and the two-sample case. This means
that we can use the likelihoods that we defined in the previous section,
and we just need to add the priors.</p>
<p>In the first one sample case, we found a <em>d</em> of 0.23 with a sample size
of 80. For our alternative hypothesis, we’ll use the narrower Cauchy
distribution. That is, a Cauchy with a location of 0 and a scale of 0.707.
And for the null hypothesis we’ll use a point at 0.</p>
<p>I’ll use the <a href="https://bayesplay.mindsci.net">bayesplay web-app</a> to define
the model. The setting’s are just as follows.</p>
<p>First the likelihood:</p>
<p><img src="https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_likelihood.png" /></p>
<p>Then the alternative prior:</p>
<p><img src="https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_alt.png" /></p>
<p>And then the null prior:</p>
<p><img src="https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_null.png" /></p>
<p>With these values entered, I can generate the R code. This code is shown
below.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="choosing-priors-part-i.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define likelihood</span></span>
<span id="cb23-2"><a href="choosing-priors-part-i.html#cb23-2" aria-hidden="true" tabindex="-1"></a>data_model <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(<span class="at">family =</span> <span class="st">&quot;noncentral_d&quot;</span>, <span class="at">d =</span> <span class="fl">0.23</span>, <span class="at">n =</span> <span class="dv">80</span>)</span>
<span id="cb23-3"><a href="choosing-priors-part-i.html#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="choosing-priors-part-i.html#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># define alternative prior</span></span>
<span id="cb23-5"><a href="choosing-priors-part-i.html#cb23-5" aria-hidden="true" tabindex="-1"></a>alt_prior <span class="ot">&lt;-</span> <span class="fu">prior</span>(<span class="at">family =</span> <span class="st">&quot;cauchy&quot;</span>, <span class="at">location =</span> <span class="dv">0</span>, <span class="at">scale =</span> <span class="fl">0.707</span>)</span>
<span id="cb23-6"><a href="choosing-priors-part-i.html#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="choosing-priors-part-i.html#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># define null prior</span></span>
<span id="cb23-8"><a href="choosing-priors-part-i.html#cb23-8" aria-hidden="true" tabindex="-1"></a>null_prior <span class="ot">&lt;-</span> <span class="fu">prior</span>(<span class="at">family =</span> <span class="st">&quot;point&quot;</span>, <span class="at">point =</span> <span class="dv">0</span>)</span>
<span id="cb23-9"><a href="choosing-priors-part-i.html#cb23-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-10"><a href="choosing-priors-part-i.html#cb23-10" aria-hidden="true" tabindex="-1"></a><span class="co"># weight likelihood by prior</span></span>
<span id="cb23-11"><a href="choosing-priors-part-i.html#cb23-11" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> data_model <span class="sc">*</span> alt_prior</span>
<span id="cb23-12"><a href="choosing-priors-part-i.html#cb23-12" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> data_model <span class="sc">*</span> null_prior</span>
<span id="cb23-13"><a href="choosing-priors-part-i.html#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="choosing-priors-part-i.html#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="co"># take the intergal of each weighted likelihood</span></span>
<span id="cb23-15"><a href="choosing-priors-part-i.html#cb23-15" aria-hidden="true" tabindex="-1"></a><span class="co"># and divide them</span></span>
<span id="cb23-16"><a href="choosing-priors-part-i.html#cb23-16" aria-hidden="true" tabindex="-1"></a>bf <span class="ot">&lt;-</span> <span class="fu">integral</span>(m1) <span class="sc">/</span> <span class="fu">integral</span>(m0)</span>
<span id="cb23-17"><a href="choosing-priors-part-i.html#cb23-17" aria-hidden="true" tabindex="-1"></a>bf</span></code></pre></div>
<pre><code>## 0.9064552</code></pre>
<p>And now we can give a bit of a description of our result.</p>
<p>The Bayes factor is 0.91. This means that the
data are 0.91 times more likely under our alternative hypothesis
relative to our null hypothesis.</p>
<p>With the one-sample case out of the way, we can now turn our attention to
the two sample case. We’ll use the same priors as before, but now we’ll
use the <strong>noncentral_d2</strong> likelihood that we used to model this data in
the previous section. Because we’re using the same priors as before, we
can just update our likelihood from the previous chunk of code, and keep
everything else the same. For this new likelihood, we’ll have a <em>d</em> of
0.99 and sample sizes of 13 and 12.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="choosing-priors-part-i.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define likelihood</span></span>
<span id="cb25-2"><a href="choosing-priors-part-i.html#cb25-2" aria-hidden="true" tabindex="-1"></a>data_model <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(</span>
<span id="cb25-3"><a href="choosing-priors-part-i.html#cb25-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;noncentral_d2&quot;</span>,</span>
<span id="cb25-4"><a href="choosing-priors-part-i.html#cb25-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">d =</span> <span class="fl">0.99</span>,</span>
<span id="cb25-5"><a href="choosing-priors-part-i.html#cb25-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">n1 =</span> <span class="dv">13</span>,</span>
<span id="cb25-6"><a href="choosing-priors-part-i.html#cb25-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">n2 =</span> <span class="dv">12</span></span>
<span id="cb25-7"><a href="choosing-priors-part-i.html#cb25-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-8"><a href="choosing-priors-part-i.html#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="choosing-priors-part-i.html#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># define alternative prior</span></span>
<span id="cb25-10"><a href="choosing-priors-part-i.html#cb25-10" aria-hidden="true" tabindex="-1"></a>alt_prior <span class="ot">&lt;-</span> <span class="fu">prior</span>(<span class="at">family =</span> <span class="st">&quot;cauchy&quot;</span>, <span class="at">location =</span> <span class="dv">0</span>, <span class="at">scale =</span> <span class="fl">0.707</span>)</span>
<span id="cb25-11"><a href="choosing-priors-part-i.html#cb25-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-12"><a href="choosing-priors-part-i.html#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># define null prior</span></span>
<span id="cb25-13"><a href="choosing-priors-part-i.html#cb25-13" aria-hidden="true" tabindex="-1"></a>null_prior <span class="ot">&lt;-</span> <span class="fu">prior</span>(<span class="at">family =</span> <span class="st">&quot;point&quot;</span>, <span class="at">point =</span> <span class="dv">0</span>)</span>
<span id="cb25-14"><a href="choosing-priors-part-i.html#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="choosing-priors-part-i.html#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="co"># weight likelihood by prior</span></span>
<span id="cb25-16"><a href="choosing-priors-part-i.html#cb25-16" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> data_model <span class="sc">*</span> alt_prior</span>
<span id="cb25-17"><a href="choosing-priors-part-i.html#cb25-17" aria-hidden="true" tabindex="-1"></a>m0 <span class="ot">&lt;-</span> data_model <span class="sc">*</span> null_prior</span>
<span id="cb25-18"><a href="choosing-priors-part-i.html#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="choosing-priors-part-i.html#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co"># take the intergal of each weighted likelihood</span></span>
<span id="cb25-20"><a href="choosing-priors-part-i.html#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="co"># and divide them</span></span>
<span id="cb25-21"><a href="choosing-priors-part-i.html#cb25-21" aria-hidden="true" tabindex="-1"></a>bf <span class="ot">&lt;-</span> <span class="fu">integral</span>(m1) <span class="sc">/</span> <span class="fu">integral</span>(m0)</span>
<span id="cb25-22"><a href="choosing-priors-part-i.html#cb25-22" aria-hidden="true" tabindex="-1"></a>bf</span></code></pre></div>
<pre><code>## 2.976933</code></pre>
<p>And now a description of our result.</p>
<p>The Bayes factor is 2.98. This means that the
data are 2.98 times more likely under our alternative hypothesis
relative to our null hypothesis.</p>
</div>
<div id="interpreting-our-bayes-factors" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> Interpreting our Bayes factors</h2>
<p>Now that we have both Bayes factors let’s think a little bit about what
they actually mean. What the Bayes factors did was compare two hypotheses.
The first hypothesis (our null hypothesis) said that the effect
size—that is, the difference in, for example, the accuracy of
remembering words in the two conditions—was 0. The second hypothesis
said that the effect size was <strong>not</strong> 0. But more specifically, it said
the effect size was <strong>not</strong> 0 in the specific way as described by the
specific Cauchy prior that we used.</p>
<p>This Cauchy prior says that we think that if there is an effect, that it
is probably somewhere between about -2.2 and 2.2 (that is, 80% of the
prior distribution lies between these values. <a href="https://osf.io/5geqt/download">Schmalz et al,
2021</a> provides a handy table that tells you
the 50% and 80% bounds for Cauchy priors of different scales (labelled
JASP scale factor on the table).</p>
<p><img src="https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/xenia_table.png" /></p>
<p>However, if you’re comfortable with <code>R</code> then you can work it out yourself.
For example, the code below calculates how much of the defined prior is
between -2 and 2.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="choosing-priors-part-i.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define the prior</span></span>
<span id="cb27-2"><a href="choosing-priors-part-i.html#cb27-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">prior</span>(<span class="st">&quot;cauchy&quot;</span>, <span class="dv">0</span>, .<span class="dv">707</span>)</span>
<span id="cb27-3"><a href="choosing-priors-part-i.html#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="choosing-priors-part-i.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># work out how much of it is between -2 and 2</span></span>
<span id="cb27-5"><a href="choosing-priors-part-i.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">integrate</span>(<span class="fu">Vectorize</span>(p<span class="sc">$</span>prior_function), <span class="sc">-</span><span class="fl">2.2</span>, <span class="fl">2.2</span>)<span class="sc">$</span>value</span></code></pre></div>
<pre><code>## [1] 0.8020498</code></pre>
<p>The Bayes factor calculation sets up two hypotheses about what we think
about the effect size in the case that there is no effect (the null) and
in the case that we think there is an effect (the alternative) and tells
us under which of these two scenarios we’d be more likely to observe our
data.</p>
<p>Note, however, that the two hypotheses that we compared are only two out
of a possible infinite set of hypotheses. I might, for example, think that
if there is an effect then it is <strong>not</strong> zero in a different way. I might,
for example, think that if there is an effect then it will be greater then
0 in a specific way as described by my prior. That is, I might one to
perform a one-sided test rather than a two-sided (or two-tailed in
frequentist terms) test.</p>
<p>To do this, all I would need to do is update my prior. In the web-app
I can do this by toggling the limit switches and setting the lower limit
to 0. As you can see the Cauchy prior is now cut in half so that it only
contains values greater than 0.</p>
<p><img src="https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/truncate.png" /></p>
<p>If we were to generate the <code>R</code> code, we’d see that the alternative prior
is now defined as follows:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="choosing-priors-part-i.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define alternative prior</span></span>
<span id="cb29-2"><a href="choosing-priors-part-i.html#cb29-2" aria-hidden="true" tabindex="-1"></a>alt_prior <span class="ot">&lt;-</span> <span class="fu">prior</span>(</span>
<span id="cb29-3"><a href="choosing-priors-part-i.html#cb29-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">family =</span> <span class="st">&quot;cauchy&quot;</span>,</span>
<span id="cb29-4"><a href="choosing-priors-part-i.html#cb29-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">location =</span> <span class="dv">0</span>,</span>
<span id="cb29-5"><a href="choosing-priors-part-i.html#cb29-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">scale =</span> <span class="fl">0.707</span>,</span>
<span id="cb29-6"><a href="choosing-priors-part-i.html#cb29-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">range =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="cn">Inf</span>)</span>
<span id="cb29-7"><a href="choosing-priors-part-i.html#cb29-7" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>In fact, I could test any arbitrary sets of hypotheses I want. In the next
section, on informed or subjective priors we’ll see how we can compare any
arbitrary set of hypotheses we want.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moving-beyond-coin-flips-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="choosing-priors-part-ii.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": [["bayes_notes.pdf", "pdf"]],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

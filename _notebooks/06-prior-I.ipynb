{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02be4206",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "name": "setup",
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = FALSE)\n",
    "suppressMessages(expr =  {\n",
    "  if (\"xfun\" %in% row.names(installed.packages()) == FALSE) {\n",
    "    install.packages(\"xfun\")\n",
    "  }\n",
    "\n",
    "display_markdown <- \\(x) IRdisplay::display_markdown(as.character(x))\n",
    "display_html <- \\(x) IRdisplay::display_html(as.character(x))\n",
    "\n",
    "xfun::pkg_attach(\n",
    "    c(\"tidyverse\",\n",
    "      \"polspline\",\n",
    "      \"patchwork\",\n",
    "      \"magrittr\",\n",
    "      \"bayesplay\",\n",
    "      \"knitr\",\n",
    "      \"broom\",\n",
    "      \"bayesplay\"),\n",
    "      install = TRUE)\n",
    "\n",
    "})\n",
    "\n",
    "table_format <- \"html\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38530ef",
   "metadata": {},
   "source": [
    "# Choosing priors\n",
    "\n",
    "In the previous section, we learned how we can use normal, student *t*,\n",
    "and various kinds of *non-central t* likelihoods to model means, mean\n",
    "differences, and effect sizes. But if we actually want to compute Bayes\n",
    "factors then we'll also need to define **priors**. While the\n",
    "**likelihoods** are a model of our **data** the **priors** will serve as\n",
    "the models for the **hypotheses** we actually want to compare.\n",
    "\n",
    "There are two broad schools of thought when it comes to defining priors.\n",
    "The first is to choose priors that can be used in a wide range of\n",
    "situations and don't need to be tailored to the specifics of the situation\n",
    "at hand. This is often framed in terms of selecting priors that aren't\n",
    "dependent on the individual beliefs or theories of a specific researcher,\n",
    "or priors that represent ignorance about any possible effect. These are\n",
    "sometimes called objective, reference, uninformative, or default priors\n",
    "(these terms aren't exactly synonymous, but for our purposes the technical\n",
    "differences won't matter). \n",
    "\n",
    "The second approach is to choose priors that are specific to the situation\n",
    "at hand. This might be by selecting priors that represent actual\n",
    "scientific theories, selecting priors that constrain the predicted effects\n",
    "to be within the expected range, or choosing priors based on, for example,\n",
    "previous evidence about the nature of the effect being studied. These\n",
    "kinds of priors go under the label of informed, or subjective priors. It\n",
    "is also important to note that the lines between the two approaches is not\n",
    "always clear cut. Rather, they are often blurred.\n",
    "\n",
    "## Reference, objective, uninformative, and default priors\n",
    "\n",
    "The most straightforward way to come up with a prior that can work in\n",
    "a wide range of situations is to use the *principle of indifference*. This\n",
    "is the approach that we used when we were coming up with our very first\n",
    "prior for the coin flip example. Our reasoning was roughly as follows:\n",
    "\n",
    "- If we don't know what the coin bias is (just that it is some value\n",
    "  between 0 and 1), then we have no reason for predicting that any\n",
    "  particular outcome (i.e., number of heads after a particular number of\n",
    "  flips) will occur more often than any other particular outcome.\n",
    "\n",
    "- If we flip the coin $n$ times, then there are $n + 1$ possible\n",
    "  outcomes. Therefore, we assign a probability of $\\frac{1}{n+1}$ to each\n",
    "  outcome. \n",
    "\n",
    "- The prior that fits with this prediction is a uniform prior from 0 to 1.\n",
    "\n",
    "The idea here is that in coming up with the prior we're trying to make as\n",
    "few assumptions as possible. Coming up with priors that make as few\n",
    "assumptions as possible is not always straightforward. There are a number\n",
    "of technical difficulties that can arise when choosing priors that\n",
    "*seemingly* don't make any assumptions. Some of these issues arise when,\n",
    "for example, choosing a prior that is non-informative when a question is\n",
    "asked one way (for example, asking about the *bias* of the coin) but then\n",
    "doesn't turn out to be non-informative when the question is asked in\n",
    "a different, but equivalent way (for example, asking about the *log odds*\n",
    "of obtaining heads). \n",
    "\n",
    "Because of these technical difficulties, people have come up with rules\n",
    "for choosing priors that make as few assumptions as possible. Once such\n",
    "rule if Jeffrey's rule. A detailed treatment is Jeffrey's rule is outside\n",
    "the scope of this course, but it is interesting to note that Jeffrey's\n",
    "rule relies on the *realm of possible events* (the same thing that caused\n",
    "our worries about *p*-values being impacted by different sampling rules). \n",
    "\n",
    "## Default priors for effect sizes\n",
    "\n",
    "Another method for defining **objective** priors that has been\n",
    "particularly popular within psychology has been to use **default priors**.\n",
    "The most prominent example of this approach has been the use of **default\n",
    "priors** for effect sizes---the so-called *default Bayesian t-test*\n",
    "[(Rouder et al,\n",
    "2009)](http://pcl.missouri.edu/sites/default/files/Rouder.bf_.pdf).\n",
    "\n",
    "The *default Bayesian t-test* can be used anywhere where a regular\n",
    "frequentist *t*-test can be used. For the default Bayes *t*-test, the data\n",
    "are modelled in terms of the effect size. That is, a *non-central d* or\n",
    "*non-central d2* likelihood is used (depending on whether the data are\n",
    "from one-sample/paired data or independent samples). These are the\n",
    "likelihood's that we defined near the end of the previous section.\n",
    "However, what really characterises this approach is the prior that is\n",
    "employed. The default Bayes *t*-test uses a **Cauchy** prior. A **Cauchy**\n",
    "distribution is similar in shape to a standard normal distribution (panel\n",
    "A below), however it has far fatter tails (panel B below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal <- prior(\"normal\", 0, 1)\n",
    "cauchy <- prior(\"cauchy\", 0, 1)\n",
    "((plot(normal) +\n",
    "  theme_minimal(14) +\n",
    "  NULL) /\n",
    "  (plot(cauchy) +\n",
    "    theme_minimal(14) +\n",
    "    NULL) +\n",
    "  plot_annotation(tag_levels = \"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc92424f",
   "metadata": {},
   "source": [
    "For a more in-depth discussion of Cauchy priors, a recent paper by\n",
    "[Schmalz et al, 2021](https://osf.io/5geqt/download) is highly\n",
    "recommended. We'll learn about them by exploring some of their properties\n",
    "using `bayesplay`. As you can see from the plots above, compared to\n",
    "a normal distribution, the Cauchy has far less mass in the middle of the\n",
    "distribution. For the Cauchy distribution, 50% of the distribution lies\n",
    "between -1 and +1 while for the normal distribution 68% of the\n",
    "distribution lies between -1 and +1. \n",
    "\n",
    "We can define a **Cauchy** prior using the `prior` function from\n",
    "`bayesplay` and setting the **family** to **cauchy**. Two other values can\n",
    "also be set. The first is **location** which determines the centre of the\n",
    "distribution. This has a default value of 0. The second is **scale** which\n",
    "can change how wide or narrow the distribution is. The original paper by\n",
    "Rouder et al (2009) set this value to 1. However, now a value of\n",
    "$\\frac{1}{\\sqrt{2}}\\approx0.707$ is more typical, and this is the default\n",
    "value in their R package (called `BayesFactor`).\n",
    "\n",
    "Let us define a Cauchy prior with a location of 0, and a scale of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "standard_cauchy <- prior(\n",
    "  family = \"cauchy\",\n",
    "  location = 0,\n",
    "  scale = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc353f",
   "metadata": {},
   "source": [
    "And now we'll define a Cauchy prior with a location of 0, and a scale of \n",
    "$\\frac{1}{\\sqrt{2}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc12a81",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "medium_cauchy <- prior(\n",
    "  family = \"cauchy\",\n",
    "  location = 0,\n",
    "  scale = 1 / sqrt(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4cb55b",
   "metadata": {},
   "source": [
    "With both priors defined we can plot them above each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "standard_cauchy_plot <- plot(standard_cauchy) +\n",
    "  theme_minimal(14) +\n",
    "  theme(title = element_text(size = 8)) +\n",
    "  labs(subtitle = \"Cauchy(0, 1)\")\n",
    "\n",
    "\n",
    "medium_cauchy_plot <- plot(medium_cauchy) +\n",
    "  theme_minimal(14) +\n",
    "  theme(title = element_text(size = 8)) +\n",
    "  labs(subtitle = \"Cauchy(0, 0.707)\")\n",
    "\n",
    "\n",
    "standard_cauchy_plot / medium_cauchy_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2edeb72",
   "metadata": {},
   "source": [
    "Although the motivation behind the default Bayes *t*-test is to come up\n",
    "with objective priors, Rouder et al (2009) also note that re-scaling the\n",
    "prior to be wider or narrower, depending on the range of predicted effect\n",
    "sizes, can be a way to *tune* the prior to the particulars of the\n",
    "experiment. As mentioned earlier, the divide between **objective** and\n",
    "**subjective** priors is a blurry one.\n",
    "\n",
    "One of these Cauchy priors is going to represent our *alternative\n",
    "hypotheses*, but we also need a prior to represent our *null hypothesis*.\n",
    "To keep things simple we'll just use a point hypothesis at zero.\n",
    "\n",
    "Now that we've decided on the priors we're going to use, we need to get to\n",
    "the most important bit! The data. We'll analyse the data from the previous\n",
    "section. We'll do both the one-sample, and the two-sample case. This means\n",
    "that we can use the likelihoods that we defined in the previous section,\n",
    "and we just need to add the priors.\n",
    "\n",
    "In the first one sample case, we found a *d* of 0.23 with a sample size\n",
    "of 80. For our alternative hypothesis, we'll use the narrower Cauchy\n",
    "distribution. That is, a Cauchy with a location of 0 and a scale of 0.707.\n",
    "And for the null hypothesis we'll use a point at 0. \n",
    "\n",
    "I'll use the [bayesplay web-app](https://bayesplay.mindsci.net) to define\n",
    "the model. The setting's are just as follows.\n",
    "\n",
    "First the likelihood:\n",
    "\n",
    "![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_likelihood.png)\n",
    "\n",
    "Then the alternative prior:\n",
    "\n",
    "![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_alt.png)\n",
    "\n",
    "And then the null prior:\n",
    "\n",
    "![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/bp_prior_null.png)\n",
    "\n",
    "With these values entered, I can generate the R code. This code is shown\n",
    "below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include = TRUE, echo = TRUE\n",
    "# define likelihood\n",
    "data_model <- likelihood(family = \"noncentral_d\", d = 0.23, n = 80)\n",
    "\n",
    "# define alternative prior\n",
    "alt_prior <- prior(family = \"cauchy\", location = 0, scale = 0.707)\n",
    "\n",
    "# define null prior\n",
    "null_prior <- prior(family = \"point\", point = 0)\n",
    "\n",
    "# weight likelihood by prior\n",
    "m1 <- data_model * alt_prior\n",
    "m0 <- data_model * null_prior\n",
    "\n",
    "# take the intergal of each weighted likelihood\n",
    "# and divide them\n",
    "bf <- integral(m1) / integral(m0)\n",
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eca01c",
   "metadata": {},
   "source": [
    "And now we can give a bit of a description of our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue::glue(\"The Bayes factor is {round(bf,2)}. This means that the\n",
    "data are {round(bf,2)} times more likely under our alternative hypothesis\n",
    "relative to our null hypothesis.\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e18c4",
   "metadata": {},
   "source": [
    "With the one-sample case out of the way, we can now turn our attention to\n",
    "the two sample case. We'll use the same priors as before, but now we'll\n",
    "use the **noncentral_d2** likelihood that we used to model this data in\n",
    "the previous section. Because we're using the same priors as before, we\n",
    "can just update our likelihood from the previous chunk of code, and keep\n",
    "everything else the same. For this new likelihood, we'll have a *d* of\n",
    "0.99 and sample sizes of 13 and 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include = TRUE, echo = TRUE\n",
    "# define likelihood\n",
    "data_model <- likelihood(\n",
    "  family = \"noncentral_d2\",\n",
    "  d = 0.99,\n",
    "  n1 = 13,\n",
    "  n2 = 12\n",
    ")\n",
    "\n",
    "# define alternative prior\n",
    "alt_prior <- prior(family = \"cauchy\", location = 0, scale = 0.707)\n",
    "\n",
    "# define null prior\n",
    "null_prior <- prior(family = \"point\", point = 0)\n",
    "\n",
    "# weight likelihood by prior\n",
    "m1 <- data_model * alt_prior\n",
    "m0 <- data_model * null_prior\n",
    "\n",
    "# take the intergal of each weighted likelihood\n",
    "# and divide them\n",
    "bf <- integral(m1) / integral(m0)\n",
    "bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8bf99d",
   "metadata": {},
   "source": [
    "And now a description of our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb399cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue::glue(\"The Bayes factor is {round(bf,2)}. This means that the\n",
    "data are {round(bf,2)} times more likely under our alternative hypothesis\n",
    "relative to our null hypothesis.\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c610a",
   "metadata": {},
   "source": [
    "## Interpreting our Bayes factors\n",
    "\n",
    "Now that we have both Bayes factors let's think a little bit about what\n",
    "they actually mean. What the Bayes factors did was compare two hypotheses.\n",
    "The first hypothesis (our null hypothesis) said that the effect\n",
    "size---that is, the difference in, for example, the accuracy of\n",
    "remembering words in the two conditions---was 0. The second hypothesis\n",
    "said that the effect size was **not** 0. But more specifically, it said\n",
    "the effect size was **not** 0 in the specific way as described by the\n",
    "specific Cauchy prior that we used. \n",
    "\n",
    "This Cauchy prior says that we think that if there is an effect, that it\n",
    "is probably somewhere between about -2.2 and 2.2 (that is, 80% of the\n",
    "prior distribution lies between these values. [Schmalz et al,\n",
    "2021](https://osf.io/5geqt/download) provides a handy table that tells you\n",
    "the 50% and 80% bounds for Cauchy priors of different scales (labelled\n",
    "JASP scale factor on the table).\n",
    "\n",
    "![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/xenia_table.png)\n",
    "\n",
    "However, if you're comfortable with `R` then you can work it out yourself. \n",
    "For example, the code below calculates how much of the defined prior is\n",
    "between -2 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa1d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "# define the prior\n",
    "p <- prior(\"cauchy\", 0, .707)\n",
    "\n",
    "# work out how much of it is between -2 and 2\n",
    "integrate(Vectorize(p$prior_function), -2.2, 2.2)$value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4434da0f",
   "metadata": {},
   "source": [
    "The Bayes factor calculation sets up two hypotheses about what we think\n",
    "about the effect size in the case that there is no effect (the null) and\n",
    "in the case that we think there is an effect (the alternative) and tells\n",
    "us under which of these two scenarios we'd be more likely to observe our\n",
    "data.\n",
    "\n",
    "Note, however, that the two hypotheses that we compared are only two out\n",
    "of a possible infinite set of hypotheses. I might, for example, think that\n",
    "if there is an effect then it is **not** zero in a different way. I might,\n",
    "for example, think that if there is an effect then it will be greater then\n",
    "0 in a specific way as described by my prior. That is, I might one to\n",
    "perform a one-sided test rather than a two-sided (or two-tailed in\n",
    "frequentist terms) test. \n",
    "\n",
    "To do this, all I would need to do is update my prior. In the web-app\n",
    "I can do this by toggling the limit switches and setting the lower limit\n",
    "to 0. As you can see the Cauchy prior is now cut in half so that it only\n",
    "contains values greater than 0.\n",
    "\n",
    "![](https://raw.githubusercontent.com/ljcolling/bayes2022/main/_site/truncate.png)\n",
    "\n",
    "If we were to generate the `R` code, we'd see that the alternative prior\n",
    "is now defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include = TRUE, echo = TRUE\n",
    "\n",
    "# define alternative prior\n",
    "alt_prior <- prior(\n",
    "  family = \"cauchy\",\n",
    "  location = 0,\n",
    "  scale = 0.707,\n",
    "  range = c(0, Inf)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9136d74",
   "metadata": {},
   "source": [
    "In fact, I could test any arbitrary sets of hypotheses I want. In the next\n",
    "section, on informed or subjective priors we'll see how we can compare any\n",
    "arbitrary set of hypotheses we want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

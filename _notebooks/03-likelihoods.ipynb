{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29838c2c",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "setup",
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = FALSE)\n",
    "suppressMessages(expr =  {\n",
    "  if (\"xfun\" %in% row.names(installed.packages()) == FALSE) {\n",
    "    install.packages(\"xfun\")\n",
    "  }\n",
    "\n",
    "  if (\"IRdisplay\" %in% row.names(installed.packages()) == TRUE) {\n",
    "    display_markdown <<- \\(x) IRdisplay::display_markdown(as.character(x))\n",
    "    display_html <<- \\(x) IRdisplay::display_html(as.character(x))\n",
    "  } else {\n",
    "    display_markdown <<- knitr::asis_output\n",
    "    display_html <<- knitr::asis_output\n",
    "  }\n",
    "\n",
    "xfun::pkg_attach(\n",
    "    c(\"tidyverse\",\n",
    "      \"polspline\",\n",
    "      \"patchwork\",\n",
    "      \"magrittr\",\n",
    "      \"bayesplay\",\n",
    "      \"knitr\",\n",
    "      \"bayesplay\"),\n",
    "      install = TRUE)\n",
    "\n",
    "})\n",
    "\n",
    "table_format <- \"html\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd63584",
   "metadata": {},
   "source": [
    "# An alternative to *p* values\n",
    "\n",
    "Coming up with an alternative to *p* values requires us to rearrange our\n",
    "thinking a bit. So let's first get straight what we're doing with frequentist\n",
    "inference. In frequentist inference we set some parameter to a certain value\n",
    "($\\theta$), we then generate data from imaginary experiments using that\n",
    "parameter setting, and we then compare our data to the data from those\n",
    "experiments. We then ask the question: \"Given that parameter value, how\n",
    "surprising is our data?\" At no point are we making inferences *about the value*\n",
    "of $\\theta$. We **set** the value, and we ask a question about our data in\n",
    "relation to **all the possible data** that might be generated.\n",
    "\n",
    "To think about what an alternative might look like, let us think back to our\n",
    "earlier example on the different meanings of probability. With *p*-values we\n",
    "thought about probability in terms of relative frequency. We were asking \"how\n",
    "often?\" questions. But I also mentioned another example. The example of being\n",
    "90% sure that the accused committed a crime. If we want to be rational humans,\n",
    "when we make claims like this what we usually do is examine the evidence. We\n",
    "**compare** whether there is more evidence for the accused's guilt or the\n",
    "accused's innocence. That is, we take the courtroom evidence and examine\n",
    "whether it supports hypothesis 1 (the accused is guilty) or hypothesis 2 (the\n",
    "accused is innocent). To do this we balance of probabilities. Is is more\n",
    "probable that we'd see this evidence if hypothesis 1 was true, or is it more\n",
    "probable that we'd see this evidence if hypothesis 2 was true? (In a civil\n",
    "trial we'd just weigh up the probabilities, but in a criminal trial we'd have\n",
    "to also examine whether this difference in probabilities exceeds some\n",
    "threshold. We'll leave this issue of thresholds for now). Might we be able to\n",
    "apply the same kind of thinking to statistical evidence?\n",
    "\n",
    "To understand the concept of statistical evidence, let's go back to our coin\n",
    "flipping example. In our coin flipping example, we collected 10 flips and found\n",
    "8 heads and 2 tails. Our frequentist analysis asked something like, \"is this\n",
    "data surprising?\". But we could ask another question. That question might go\n",
    "something like this: \"Is it more likely that the **bias is 0.6** or that the\n",
    "**bias is 0.8** given that we'd obtained 8 heads in 10 flips?\"\n",
    "\n",
    "To try and answer this question, we'll again create some simulations. We'll\n",
    "start by creating two **sampling distributions**. For now we'll keep things\n",
    "simple and we'll create these sampling distributions on the assumption that I\n",
    "intended to flip the coin 10 times. To create our sampling distributions we'll\n",
    "first set $\\theta$ to 0.6 and run the simulations, and then we'll set $\\theta$\n",
    "to 0.8 and run the simulations. I know the distribution they'll follow, so I'll\n",
    "just compute the distributions directly rather than actually running the\n",
    "simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd23a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip the coin n times and then count up the heads.\n",
    "# three parameters need to be set\n",
    "# 1. the number of flips (n_flips)\n",
    "# 2. the probability of heads (pr_heads)\n",
    "# 3. the number of heads in our observation (obs_heads)\n",
    "\n",
    "coin_flip_v1 <- function(n_flips, pr_heads, obs_heads) {\n",
    "  pmap_df(\n",
    "    tibble(heads = 0:n_flips, flips = n_flips, pr_heads = pr_heads),\n",
    "    function(heads, flips, pr_heads) {\n",
    "      tibble(\n",
    "        flips = flips, heads = heads,\n",
    "        freq = dbinom(heads, flips, pr_heads)\n",
    "      )\n",
    "    }\n",
    "  ) %>% # get the frequency\n",
    "    mutate(our_ob = case_when(\n",
    "      flips == n_flips & heads == obs_heads ~ TRUE,\n",
    "      TRUE ~ FALSE\n",
    "    )) # mark our observation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981a573",
   "metadata": {},
   "source": [
    "We can draw the distributions of the possible data that would occur for\n",
    "different values of P(heads) = $\\theta$. In each of the plots, our\n",
    "actual observation will be highlighted. Although we're \"simulating\" all\n",
    "possible observations, you'll see that we're only going to care about our\n",
    "**actual** observation. We will want to know the relative frequency with which\n",
    "**that** result occurs, not the frequency of results that didn't but might've\n",
    "occurred. I'm going to draw several distributions not just two that correspond\n",
    "to the values of $\\theta$ that we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a885789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw distributions of the data for various values of pr_heads for version 1\n",
    "# (flip n times)\n",
    "\n",
    "n_flips <- 10\n",
    "\n",
    "pr_heads_values <- c(0.2, 0.4, 0.6, 0.8) # set our pr_heads values\n",
    "\n",
    "obs_heads <- 8\n",
    "\n",
    "# make the plots\n",
    "coin_flip_v1_plots <- pmap(\n",
    "  tibble(n_flips = n_flips, pr_heads = pr_heads_values, obs_heads = obs_heads),\n",
    "  function(n_flips, pr_heads, obs_heads) {\n",
    "    coin_flip_v1(n_flips, pr_heads, obs_heads) %>%\n",
    "      ggplot(aes(x = heads, y = freq)) +\n",
    "      geom_line(alpha = .25) +\n",
    "      geom_point(aes(colour = our_ob), size = 3) +\n",
    "      scale_colour_manual(\n",
    "        guide = \"none\",\n",
    "        values = c(\"TRUE\" = \"black\", \"FALSE\" = \"grey\")\n",
    "      ) +\n",
    "      labs(\n",
    "        x = glue::glue(\"number of heads in {n_flips} flips\"),\n",
    "        y = \"relative frequency\",\n",
    "        title = glue::glue(\"P(heads) = {pr_heads}\")\n",
    "      ) +\n",
    "      scale_x_continuous(breaks = c(seq(0, n_flips, 1))) +\n",
    "      scale_y_continuous(limits = c(0, .5)) +\n",
    "      theme_minimal() +\n",
    "      NULL\n",
    "  }\n",
    ")\n",
    "\n",
    "# make the plots pretty and arrange them\n",
    "\n",
    "(wrap_plots(coin_flip_v1_plots, nrow = 2) +\n",
    "  plot_annotation(tag_levels = \"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2325563",
   "metadata": {},
   "source": [
    "Let's take these plots and create a new one out of them. Since we're just\n",
    "interested in **our specific observation** we'll take all the marked points and\n",
    "put them on a plot of their own. Now we'll still have relative frequency on the\n",
    "y-axis, but on the x-axis we won't have the observation anymore (because we're\n",
    "only focused on one specific outcome). Instead, we'll have $\\theta$ on the\n",
    "x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f45dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set our observation again\n",
    "number_of_heads <- 8\n",
    "number_of_flips <- 10\n",
    "\n",
    "# and set the range of bias to consider\n",
    "pr_heads_range <- seq(0, 1, .1) # 0/10, 1/10 ... 9/10, 10/10\n",
    "\n",
    "# translate our observation into the parameters needed for version 1\n",
    "# generate the data and pull out the relative frequency of our specific\n",
    "# observation\n",
    "\n",
    "n_flips <- number_of_flips\n",
    "obs_heads <- number_of_heads\n",
    "likelihood_v1 <- map_df(pr_heads_range, function(x) {\n",
    "  coin_flip_v1(n_flips, x, obs_heads) %>%\n",
    "    filter(our_ob == TRUE) %>%\n",
    "    select(freq) %>%\n",
    "    mutate(pr_heads = x)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now actually draw the plot\n",
    "\n",
    "likelihood_v1 %>%\n",
    "  ggplot(aes(x = pr_heads, y = freq)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  theme_minimal(12) +\n",
    "  scale_x_continuous(name = \"P(heads)\", breaks = seq(0, 1, .2)) +\n",
    "  scale_y_continuous(limits = c(0, .4), name = \"relative frequency\") +\n",
    "  labs(\n",
    "    title = glue::glue(\n",
    "      \"likelihood function for {number_of_heads}\",\n",
    "      \" heads in {number_of_flips} flips\"\n",
    "    ),\n",
    "    subtitle = glue::glue(\"stopping after {number_of_flips}\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d36e3",
   "metadata": {},
   "source": [
    "This new plot that we're created illustrates what's know as the **likelihood**\n",
    "function. The likelihood function describes the relationship between values of\n",
    "the parameter and **our data**. It's made up of slices of the sampling\n",
    "distribution-the slices that correspond to our actual observation. Remember\n",
    "that when we were doing inference with the sampling distribution we were\n",
    "looking at the extreme tails of the sampling distribution. That is, we were\n",
    "interested in the entire shape of the sampling distribution. Now we're instead\n",
    "only interested in the thin slice that corresponds to our observation.\n",
    "\n",
    "## Doing inference with likelihoods\n",
    "\n",
    "The likelihood plays a key role in Bayesian inference. Inferences on the basis\n",
    "of likelihoods are derived from what is known at the *law of likelihood*.\n",
    "Simply stated, the *law of likelihood* says that for a given pair of\n",
    "hypotheses---for example, $\\mathcal{H}_1$ that the coin bias is P(heads)=0.6\n",
    "and $\\mathcal{H}_2$ that the coin bias is P(heads)=0.8---then data support\n",
    "$\\mathcal{H}_1$ over $\\mathcal{H}_2$ if the likelihood of $\\mathcal{H}_1$\n",
    "exceeds that of $\\mathcal{H}_2$. Or, put another way, if our data would be\n",
    "produced more often if $\\mathcal{H}_1$ were true than if $\\mathcal{H}_2$ were\n",
    "true, then the data provide support for $\\mathcal{H}_1$ over $\\mathcal{H}_2$\n",
    "(See [Hacking,\n",
    "1965](https://www.google.co.uk/books/edition/Logic_of_Statistical_Inference/cEJfDAAAQBAJ?hl=en&gbpv=0) Chapter 5, for both formulations).\n",
    "\n",
    "This definition might seem a little opaque, but we can read these likelihood\n",
    "values straight off our likelihood plot. The height of the likelihood plot, at\n",
    "each value of $\\theta$, tells you the probability of obtaining your data given\n",
    "that value of $\\theta$. If the likelihood function is higher at $\\theta=0.8$\n",
    "than $\\theta=0.6$ then the probability of obtaining our data would be higher if\n",
    "$\\theta$ was 0.8 than it would be if $\\theta$ was 0.6. Consequently, our data\n",
    "support the hypothesis that $\\theta=0.8$ *over* the hypothesis $\\theta=0.6$. A\n",
    "key point here, that's worth stressing, is that this is a comparison between\n",
    "two specific hypothesis. Does this data support this one specific hypothesis\n",
    "over this other specific hypothesis. What you're doing here is *weighing up\n",
    "probabilities* just like you would do in a courtroom.\n",
    "\n",
    "### A brief detour back to sampling rules\n",
    "\n",
    "Before we continue, let's just go back to something from the previous section.\n",
    "I made a big deal about how our sampling rules change the shape of the sampling\n",
    "distribution, and that this then changes the inferences that we make. This is\n",
    "the case even if nothing changes about our actual data. But do different\n",
    "sampling rules change the likelihood? To test this out, we'll generate a new\n",
    "set of sampling distributions using the other sampling rule (sampling until we\n",
    "get 2 heads). And from these sampling distributions we'll generate some\n",
    "likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b92bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function v2 \"simulates\" results from the version where you\n",
    "# flip the coin until it comes up tails n times.\n",
    "# three parameters need to be set\n",
    "# 1. the number of tails to stop at (n_tails)\n",
    "# 2. the probability of heads (pr_heads)\n",
    "# 3. the number of flips in our observation (obs_flips)\n",
    "\n",
    "coin_flip_v2 <- function(n_tails, pr_heads, obs_flips) {\n",
    "  pmap_df(\n",
    "    tibble(\n",
    "      tails = n_tails,\n",
    "      pr_heads = pr_heads,\n",
    "      flips = 0:(obs_flips + 4)\n",
    "    ), # input values\n",
    "    function(tails, pr_heads, flips) {\n",
    "      tibble(\n",
    "        flips = flips, tails = tails,\n",
    "        freq = dnbinom(flips - tails, tails, 1 - pr_heads)\n",
    "      )\n",
    "    }\n",
    "  ) %>%\n",
    "    mutate(our_ob = case_when(\n",
    "      flips == obs_flips & tails == tails ~ TRUE,\n",
    "      TRUE ~ FALSE\n",
    "    )) # mark our observation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4dbdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw distributions of the data for various values of pr_heads for version 2\n",
    "# (flip until n tails)\n",
    "\n",
    "n_tails <- 2\n",
    "n_flips <- 10\n",
    "pr_heads_values <- c(0.2, 0.4, 0.6, 0.8) # set our pr_heads\n",
    "obs_flips <- 10\n",
    "# make the plots\n",
    "coin_flip_v2_plots <- pmap(\n",
    "  tibble(n_tails = n_tails, pr_heads = pr_heads_values, obs_flips = obs_flips),\n",
    "  function(n_tails, pr_heads, obs_flips) {\n",
    "    coin_flip_v2(n_tails, pr_heads, obs_flips) %>%\n",
    "      ggplot(aes(x = flips, y = freq)) +\n",
    "      geom_line(alpha = .25, na.rm = TRUE) +\n",
    "      geom_point(aes(colour = our_ob), size = 3, na.rm = TRUE) +\n",
    "      scale_colour_manual(\n",
    "        guide = \"none\",\n",
    "        values = c(\"TRUE\" = \"black\", \"FALSE\" = \"grey\")\n",
    "      ) +\n",
    "      labs(\n",
    "        x = glue::glue(\"number of flips until {n_tails} tails\"),\n",
    "        y = \"relative frequency\",\n",
    "        title = glue::glue(\"P(heads) = {pr_heads}\")\n",
    "      ) +\n",
    "      scale_x_continuous(breaks = seq(2, 12, 2), limits = c(2, 12)) +\n",
    "      scale_y_continuous(limits = c(0, .85)) +\n",
    "      theme_minimal() +\n",
    "      NULL\n",
    "  }\n",
    ")\n",
    "\n",
    "\n",
    "(wrap_plots(coin_flip_v2_plots, nrow = 2) +\n",
    "  plot_annotation(tag_levels = \"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fac880",
   "metadata": {},
   "source": [
    "We can see that these sampling distributions look very different to the\n",
    "sampling distributions that we generated above. But what we're interested in\n",
    "are just the highlighted points, because we'll use these to generate our\n",
    "likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b01071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate our observation into the parameters needed for version 2\n",
    "# generate the data and out the relative frequency of our specific observation\n",
    "n_tails <- number_of_flips - number_of_heads\n",
    "obs_flips <- number_of_flips\n",
    "likelihood_v2 <- map_df(pr_heads_range, function(x) {\n",
    "  suppressWarnings({ # supress warnings about impossible values\n",
    "    coin_flip_v2(n_tails, x, obs_flips) %>%\n",
    "      filter(our_ob == TRUE) %>%\n",
    "      select(freq) %>%\n",
    "      mutate(pr_heads = x) %>%\n",
    "      mutate(freq = ifelse(is.na(freq), 0, freq))\n",
    "  })\n",
    "}) # replace NaN (impossible values) with 0 for plotting\n",
    "\n",
    "# now actually draw the plot\n",
    "\n",
    "likelihood_v2 %>%\n",
    "  ggplot(aes(x = pr_heads, y = freq)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  theme_minimal(12) +\n",
    "  scale_x_continuous(name = \"P(heads)\", breaks = seq(0, 1, .2)) +\n",
    "  scale_y_continuous(limits = c(0, .4), name = \"relative frequency\") +\n",
    "  labs(\n",
    "    title = glue::glue(\n",
    "      \"likelihood function for {number_of_heads} \",\n",
    "      \"in {number_of_flips}\"\n",
    "    ),\n",
    "    subtitle = glue::glue(\"stopping after {n_tails} tails\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6898e",
   "metadata": {},
   "source": [
    "The new likelihood might, at first glance, look different to the one we\n",
    "generated earlier, but it's just a scaled version of the earlier likelihood. We\n",
    "can check this just by rescaling the two likelihoods so that they both have a\n",
    "max of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now draw scaled versions of the plots\n",
    "\n",
    "likelihood_v1_plot <- likelihood_v1 %>%\n",
    "  mutate(freq = freq / max(freq)) %>%\n",
    "  ggplot(aes(x = pr_heads, y = freq)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  theme_minimal(12) +\n",
    "  scale_x_continuous(name = \"P(heads)\", breaks = seq(0, 1, .2)) +\n",
    "  scale_y_continuous(limits = c(0, 1), name = \"relative frequency\") +\n",
    "  labs(\n",
    "    title = glue::glue(\n",
    "      \"likelihood function for {number_of_heads} \",\n",
    "      \"heads in {number_of_flips} flips\"\n",
    "    ),\n",
    "    subtitle = glue::glue(\"stopping after {number_of_flips}\")\n",
    "  )\n",
    "\n",
    "likelihood_v2_plot <- likelihood_v2 %>%\n",
    "  mutate(freq = freq / max(freq)) %>%\n",
    "  ggplot(aes(x = pr_heads, y = freq)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  theme_minimal(12) +\n",
    "  scale_x_continuous(name = \"P(heads)\", breaks = seq(0, 1, .2)) +\n",
    "  scale_y_continuous(limits = c(0, 1), name = \"relative frequency\") +\n",
    "  labs(\n",
    "    title = glue::glue(\n",
    "      \"likelihood function for {number_of_heads} \",\n",
    "      \"in {number_of_flips}\"\n",
    "    ),\n",
    "    subtitle = glue::glue(\"stopping after {n_tails} tails\")\n",
    "  )\n",
    "\n",
    "\n",
    "(likelihood_v1_plot / likelihood_v2_plot +\n",
    "  plot_annotation(tag_levels = \"A\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bd70f2",
   "metadata": {},
   "source": [
    "Now that they're been rescaled we can see that they're they same. Note that\n",
    "scaling changes the absolute distance between points on the likelihood, but it\n",
    "doesn't change the *relative* distance between the points. When we want to know\n",
    "the difference between two likelihood values we take the *ratio* of these two\n",
    "values. The ratio gives us the *relative distance* between the heights on the\n",
    "likelihood function, and the *relative distance* doesn't change with scaling.\n",
    "\n",
    "More importantly, however, what this demonstrates is that when we do inference\n",
    "with **likelihoods** instead of **sampling distributions**, things like\n",
    "stopping rules, data that wasn't collected but might have been collected, and\n",
    "all those other sorts of things that were tricky about *p*-values don't come in\n",
    "to play. We only have the worry about **the data we actually have**, and the\n",
    "**likelihood** which relates **parameter values** to **data**.\n",
    "\n",
    "### The likelihood ratio\n",
    "\n",
    "The likelihood ratio is going to be our measure of evidence of how much\n",
    "the data supports one hypothesis over another. If the likelihood at point\n",
    "one ($\\theta_1$) is four times the larger than the likelihood at point two\n",
    "($\\theta_2$) then the data are four times more likely under the hypothesis\n",
    "$\\theta = \\theta_1$ than the hypothesis $\\theta = \\theta_2$. Or simply\n",
    "put, the data supports the hypothesis $\\theta=\\theta_1$  over\n",
    "$\\theta=\\theta_2$ but a factor of 4 to 1.\n",
    "\n",
    "Let's look at the likelihood for our actual data and our two hypotheses\n",
    "about the coin bias. Just to drive home the point that the sampling rule\n",
    "doesn't matter, I'm going to work out the likelihood ratio for the\n",
    "sampling rule where I flip the coin 10 times and the sampling rule where\n",
    "I flip the coin until I get 2 heads and just happen to flip it 10 times.\n",
    "We'll see that the absolute values of the likelihoods change (as we saw in\n",
    "the plots above), but that the likelihood ratio between the hypotheses\n",
    "don't change. \n",
    "\n",
    "To make sure that the numbers work out correctly, I won't use simulations\n",
    "to generate the likelihoods. Instead I'll just generate each likelihood\n",
    "with the relevant formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e95522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for nice printing\n",
    "print_likelihood <- function(h1, h2, likelihood_1, likelihood_2) {\n",
    "  likelihood_1_text <- glue::glue(\n",
    "    \"The likelihood for $\\\\mathcal{{H}}_1$ (P(heads) = {h1}) \",\n",
    "    \"is {round(likelihood_1,2)}\"\n",
    "  )\n",
    "  likelihood_2_text <- glue::glue(\n",
    "    \"The likelihood for $\\\\mathcal{{H}}_2$ (P(heads) = {h2}) \",\n",
    "    \"is {round(likelihood_2,2)}\"\n",
    "  )\n",
    "  ratio_text <- glue::glue(\n",
    "    \"The likelihood ratio is {round(likelihood_1 / likelihood_2, 2)}\"\n",
    "  )\n",
    "  interpretation <- glue::glue(\n",
    "    \"The data are {round(likelihood_1 / likelihood_2, 2)} times more \",\n",
    "    \"probable under $\\\\mathcal{{H}}_1$ than $\\\\mathcal{{H}}_2$\"\n",
    "  )\n",
    "  paste(c(likelihood_1_text, likelihood_2_text, ratio_text, interpretation),\n",
    "    collapse = \"\\n\\n\"\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0281c2f9",
   "metadata": {},
   "source": [
    "First, for version 1, where I flip the coin 10 times (**binomial**\n",
    "sampling rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196ca19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set out observation\n",
    "n <- 10 # number of flips\n",
    "h <- 8 # number of heads\n",
    "t <- n - h # number of tails\n",
    "\n",
    "# set our two hypotheses\n",
    "h1 <- 0.6 # the probability of getting heads is 0.6\n",
    "h2 <- 0.8 # the probability of getting heads is 0.8\n",
    "\n",
    "# work out the likelihood values\n",
    "likelihood_1 <- dbinom(h, n, h1)\n",
    "likelihood_2 <- dbinom(h, n, h2)\n",
    "\n",
    "print_likelihood(h1, h2, likelihood_1, likelihood_2) %>% display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3bfc6",
   "metadata": {},
   "source": [
    "Second, for version 2, where I flip the coin until I get 2 hears\n",
    "(**negative-binomial** sampling rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df02ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our observation\n",
    "n <- 10 # number of flips\n",
    "h <- 8 # number of heads\n",
    "t <- n - h # number of tails\n",
    "\n",
    "# set our two hypotheses\n",
    "h1 <- 0.6 # the probability of getting heads is 0.6\n",
    "h2 <- 0.8 # the probability of getting heads is 0.8\n",
    "\n",
    "# work out the likelihood values\n",
    "likelihood_1 <- dnbinom(t, h, h1)\n",
    "likelihood_2 <- dnbinom(t, h, h2)\n",
    "\n",
    "print_likelihood(h1, h2, likelihood_1, likelihood_2) %>% display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b94b8d",
   "metadata": {},
   "source": [
    "### A note about likelihood functions and probability distributions\n",
    "\n",
    "One common misconception about likelihood functions is that they're\n",
    "probability distributions. This misconception can come in a few different\n",
    "forms, so it's worth just stressing again what a likelihood function is. \n",
    "\n",
    "First, we can tell a likelihood function isn't a probability distribution,\n",
    "because for a probability distribution the area under the curve would have\n",
    "to sum to 1. Each point on a probability distribution gives the probability\n",
    "of a specific event. The whole curve describes all the events that could\n",
    "happen, and the area under the curve gives the probability that one of the\n",
    "possible events happens. That is, it is the sum of all the individual\n",
    "probabilities of the different events.\n",
    "\n",
    "In the plot below, we can see the likelihood functions for different\n",
    "events (different numbers of heads in 10 flips). We can see that the area\n",
    "under the curve varies in each case. If these were probability\n",
    "distributions then the area under the curve in each case would be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2752c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some likelihood functions\n",
    "\n",
    "scenarios <- list(\n",
    "  p1 = list(heads = 0, flips = 10),\n",
    "  p2 = list(heads = 1, flips = 2),\n",
    "  p3 = list(heads = 0, flips = 2),\n",
    "  p4 = list(heads = 2, flips = 5)\n",
    ")\n",
    "\n",
    "plots <- map(scenarios, function(x) {\n",
    "  data_model <- bayesplay::likelihood(\"binomial\",\n",
    "    successes = x$heads,\n",
    "    trials = x$flips\n",
    "  )\n",
    "  auc <- integrate(data_model$likelihood_function, 0, 1)$value\n",
    "  plot(data_model) +\n",
    "    labs(\n",
    "      title = glue::glue(\"likelihood for {x$heads} heads in {x$flips} flips\"),\n",
    "      subtitle = glue::glue(\"area under curve is {round(auc,3)}\")\n",
    "    ) +\n",
    "    theme_minimal(12) +\n",
    "    NULL\n",
    "})\n",
    "\n",
    "wrap_plots(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9414d",
   "metadata": {},
   "source": [
    "This misconception about likelihood functions being probability\n",
    "distribution often takes the form of thinking that the likelihood function\n",
    "tells use the probability of the parameter being a specific value. That\n",
    "is, it tells us that there's a higher probability that $\\theta=\\theta_1$\n",
    "than $\\theta=\\theta_2$, given our data. Put another way, this\n",
    "misconception states that the likelihood tells us $p(\\theta|y)$. This\n",
    "quantity, however, is what's know as the **posterior probability**.\n",
    "Rather, the likelihood tells us the reserve conditional, or $p(y|\\theta)$.\n",
    "That is, it tells use the probability of obtaining our data given\n",
    "different values of the parameter.\n",
    "\n",
    "To emphasise that the likelihood is not a probability distribution it is\n",
    "often denoted $\\mathcal{L}(\\theta|y)$.\n",
    "\n",
    "## Testing more complex hypotheses\n",
    "\n",
    "So we've seen that comparing likelihoods (by taking their ratio) can tell\n",
    "us which hypothesis is better supported by the data. However, there's\n",
    "a couple of problems with what we've done up until now. First, how do we\n",
    "know explicitly set a threshold for when we would start digging for\n",
    "treasure. Is there also a threshold for likelihood ratios? To answer this\n",
    "question, we're going to have to take into account a lot of additional\n",
    "factors. And the answer to this question is probably going to be\n",
    "context-dependent. For example, if we're placing bets on hypotheses, we're\n",
    "probably going to want to take into account the relative pay-offs. If\n",
    "we're using evidence to decide somebody's guilt in a court case, we're\n",
    "probably going to want to take into account things like \"reasonable\n",
    "doubt\". In short, there's not a straight forward answer to this question,\n",
    "so we'll set it aside for now. Instead, we'll turn to the second problem.\n",
    "\n",
    "The second problem with what we've done up until now is that we've just\n",
    "been comparing single point hypothesis. We can can say, for example,\n",
    "whether the data supports P(heads) = 0.5 over the hypothesis P(heads)\n",
    "= 0.8, and we can quantify this level of support. But usually, we are not\n",
    "comparing two simple hypotheses like this. Our hypotheses take a more\n",
    "complex form like: \"Is the coin fair?\"\n",
    "\n",
    "How might we go about answering this question?\n",
    "\n",
    "To come up with a way to answer this question we're going to think about\n",
    "hypotheses in terms of **predictions**. Our first hypothesis,\n",
    "$\\mathcal{H}_0$, will be that the coin is fair. And we'll say a fair coin\n",
    "has a bias of 0.5. What do we predict will happen if we flip the coins 10\n",
    "times? Most of the time it'll show around about 5 heads and 5 tails, but\n",
    "it will also rarely show 1 head and 9 heads etc. If we plotted it, it\n",
    "would just be our sampling distribution from before. \n",
    "\n",
    "For $\\mathcal{H}_1$, that the coin isn't fair, what do we predict will\n",
    "happen if we flipped it 10 times? Before we can work this out we need to\n",
    "think a little bit about what it means for a coin not to be fair. For now,\n",
    "let's say that it means that it can have some bias between 0 and 1, but\n",
    "that we don't know what it is. For our fair coin, if we collected a very\n",
    "large number of samples the most common outcome would be 5 heads and\n",
    "5 tails, but would be the most common outcome with our unfair coin? Would\n",
    "it be 5 heads and 5 tails? Would it be 0 heads? 1 head? 9 heads? Do we\n",
    "have any grounds for **predicting** that one outcome would be more common\n",
    "than another outcome? We arguably do not. If so, then if I asked which of\n",
    "the 11 possible outcomes (from a sample of 10 coin flips) is more probable\n",
    "than the others you might say none. If none of the outcomes are more\n",
    "probable than any of the other outcomes, and given that there's 11\n",
    "possible outcomes, then our prediction must be that each outcome has\n",
    "a 1 in 11 chance of occurring.\n",
    "\n",
    "Below, we can see plots of our two predictions. First, what we would\n",
    "predict if we knew the coin bias was 0.5, and second what we would predict\n",
    "if we had no reason for favouring one outcome over another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc516d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# the two predictions\n",
    "\n",
    "predictions1 <- tibble(\n",
    "  x = seq(0, 10, 1),\n",
    "  y = dbinom(x, 10, 0.5)\n",
    ") %>%\n",
    "  ggplot(data = ., aes(x = x, y = y)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  labs(x = \"number of heads\", y = \"probability\") +\n",
    "  scale_x_continuous(breaks = seq(0, 10, 1)) +\n",
    "  theme_minimal(14)\n",
    "\n",
    "\n",
    "predictions2 <- tibble(\n",
    "  x = seq(0, 10, 1),\n",
    "  y = 1 / length(x)\n",
    ") %>%\n",
    "  ggplot(data = ., aes(x = x, y = y)) +\n",
    "  geom_point() +\n",
    "  geom_line() +\n",
    "  labs(x = \"number of heads\", y = \"probability\") +\n",
    "  scale_x_continuous(breaks = seq(0, 10, 1)) +\n",
    "  theme_minimal(14)\n",
    "\n",
    "(predictions1  / predictions2) +\n",
    "  plot_annotation(tag_levels = \"A\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418120ef",
   "metadata": {},
   "source": [
    "Now that we have a intuition for hypotheses in terms of predictions, let\n",
    "use formalise it a bit. And instead of thinking about all the data that\n",
    "might be produced let's just try and think about the probability of\n",
    "obtaining our data of 8 heads in 10 flips. If a coin is fair, then in 10\n",
    "coin flips there are exactly $2^{10}$ possible sequences and 45 of these\n",
    "sequences would give 8 heads in 10 flips. Therefore, if the coin is fair,\n",
    "then the probability of obtaining our result of 8 heads in 10 flips in\n",
    "$\\frac{45}{1024}$, or about 0.044. I've worked this out exactly, but we'd\n",
    "get the same value if we ran the simulations, or if we just looked at the\n",
    "likelihood function at $\\theta=0.5$. This is after all, what the\n",
    "likelihood function tells use: the probability of obtaining our data for\n",
    "a given value of the parameter. \n",
    "\n",
    "Now on to the more complex example where the coin bias is some unknown\n",
    "value between 0 and 1. What now is the probability of obtaining our data.\n",
    "One good strategy of dealing with unknowns is to average across the\n",
    "possibilities. For example, if I didn't know what the coin bias was, but\n",
    "I knew it could either be 0.5 or 0.6, then to work out the probability of\n",
    "obtaining our current data I could just work out the probability of\n",
    "obtaining our current data if the bias was 0.5 (~0.044), and then work out\n",
    "the probability of obtaining our current data if 0.6 (~0.121), and then\n",
    "just average them together (~0.082). Again, I just take the values from\n",
    "the likelihood function at $\\theta=0.5$ and $\\theta=0.6$ and aveage them\n",
    "together. \n",
    "\n",
    "But in our example it's not just the case that the bias of the coin could\n",
    "be 0.5 or 0.6. For our second hypothesis we said it could be any value\n",
    "between 0 and 1. That is, hypothesis is the set $\\Theta\n",
    "= \\{\\theta_1,\\theta_2,...,\\theta_n\\}$ where each $\\theta_1$ to $\\theta_n$\n",
    "is some value between 0 and 1. To keep things simple for now, we'll say\n",
    "that $\\Theta =\\{\\theta_1=\\frac{0}{10},\n",
    "\\theta_2=\\frac{1}{10},...,\\theta_{11}=\\frac{10}{10}\\}$. An average is just\n",
    "a sum where each value is multiplied by $\\frac{1}{n}$, therefore, the\n",
    "average across these 11 values would be:\n",
    "\n",
    "$$\\sum_{i=1}^{11}\\mathcal{L}(\\theta_i|\\mathbf{y})\\cdot{}\\frac{1}{11}$$\n",
    "\n",
    "This gives a value of approximately $\\frac{1}{12}$, which is pretty close\n",
    "to the value of $\\frac{1}{11}$ we worked out earlier. Why is it not the\n",
    "same? Well, earlier, we said it could be **any** value between 0 and 1.\n",
    "We're only looking at 11 values. Let's instead look at 101 values between\n",
    "0 and 1. Now $\\Theta = \\{\\theta_1=\\frac{0}{100},\n",
    "\\theta_2=\\frac{1}{100},...,\\theta_{101}=\\frac{100}{100}\\}$. Now we get\n",
    "a value that's even closer to $\\frac{1}{11}$. To get to exactly\n",
    "$\\frac{1}{11}$, however, we're going to have to look at even more points.\n",
    "Instead of spacing the points out by $\\frac{1}{10}$ or $\\frac{1}{100}$,\n",
    "we're going to need infinitesimally small spacing. That's means we just\n",
    "switch out the sum for an integral, but the logic is the same. We're still\n",
    "just taking an average.\n",
    "\n",
    "$$\\int_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|\\mathbf{y})d(\\theta)$$\n",
    "\n",
    "Now that we're taking an integral, we get exactly $\\frac{1}{11}$.\n",
    "\n",
    "Now that we have these two values: First, $\\frac{45}{1024}$, which gives the\n",
    "probability of obtaining 8 heads in 10 flips if $\\theta=0.5$, and second,\n",
    "$\\frac{1}{11}$, which the probability of obtaining 8 head in  10 flips if\n",
    "$\\theta$ was some unknown value between 0 and 1, what can we do with them?\n",
    "Well, we can just take the ratio! Just like we did with the two simple point\n",
    "hypotheses, we can also take the ratio between our simple point hypothesis and\n",
    "our more complex hypothesis. Taking this ratio tells us that's we'd be\n",
    "$2\\frac{34}{495}$ times more likely to see our data if $\\theta$ was some\n",
    "unknown value between 0 and 1 than if $\\theta=0.5$.\n",
    "\n",
    "Thinking back to the  *law of likelihood* that we covered at the start of\n",
    "this section, we said if our data would be produced more often if\n",
    "$\\mathcal{H}_1$ were true than if $\\mathcal{H}_2$ were true, then the data\n",
    "provide support for $\\mathcal{H}_1$ over $\\mathcal{H}_2$. This is exactly\n",
    "the number that we've just worked out.\n",
    "\n",
    "### There's more than one way to average\n",
    "\n",
    "Above we worked out two values. The second number we calculated by\n",
    "averaging the likelihood function, but the first number we calculated by\n",
    "just taking a single point on the likelihood function. So one involved an\n",
    "average and the other did not. Or did it? We can actually think of both as\n",
    "involving an average of the likelihood function. They're just different\n",
    "kinds of average. We can view both as taking a *weighted average*, where\n",
    "different values contribute more or less to the average. For the second\n",
    "number, all values in the average were *weighted equally*. That is, it was\n",
    "just like a regular average. For the first, it can be viewed as taking an\n",
    "average where the likelihood value for $\\theta=0.5$ is given a weight of\n",
    "1, and all over values of given a weight of 0. We could visualise these\n",
    "weighting in the plots below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03030f7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#| fig.height: 4\n",
    "\n",
    "point <- ggplot(\n",
    "  data = tibble(x = 0.5, y = 1),\n",
    "  aes(x = x, y = y)\n",
    ") +\n",
    "  geom_point() +\n",
    "  geom_segment(x = 0.5, xend = 0.5, y = 0, yend = 1) +\n",
    "  xlim(0, 1) +\n",
    "  ylim(0, 1) +\n",
    "  labs(x = \"coin bias\", y = \"weight\") +\n",
    "  theme_minimal(14)\n",
    "\n",
    "uniform <- ggplot(\n",
    "  data = tibble(x = seq(0, 1, 0.1), y = 1),\n",
    "  aes(x = x, y = y)\n",
    ") +\n",
    "  geom_line() +\n",
    "  xlim(0, 1) +\n",
    "  ylim(0, 1) +\n",
    "  labs(x = \"coin bias\", y = \"weight\") +\n",
    "  theme_minimal(14)\n",
    "\n",
    "\n",
    "\n",
    "point + uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b8ea7c",
   "metadata": {},
   "source": [
    "These *weightings* can be thought of as *probability distributions*. We\n",
    "are going to call these **priors**. Mathematically, they represents the\n",
    "weights that we apply to the values that we average together. But what do\n",
    "they represents *conceptually*. One way to think of them is that they\n",
    "*represent our beliefs about the parameter value* (in this case the *coin\n",
    "bias*). Or, that they represent our *model of the hypothesis*-that is,\n",
    "they represent what the hypothesis has to say about the parameter value.\n",
    "So the fair coin hypothesis represents is a model that says *the coin bias\n",
    "is exactly 0.5*. The other hypothesis is a model that say *all values of\n",
    "the coin bias between 0 and 1 are equally probable*. \n",
    "\n",
    "At the start of this section we said that for our alternative for a fair\n",
    "coin we'd say that all values of the coin bias were equally likely, and\n",
    "this is what we'd mean by an unfair coin. But this is only one possible\n",
    "model of an unfair coin. We might actually think that if a coin is unfair\n",
    "then it'll show heads far more often than tails. Or, we might think that\n",
    "unfair coins will show tails more often than heads. We might even think\n",
    "that unfair coins will behave very similarly to fair coins, but they'll\n",
    "just outcomes of 5 head and 5 tails a little bit less often than the\n",
    "$\\frac{252}{1024}$ that we'd see with a perfectly fair coin. These are all\n",
    "different *models* that we might have about unfair coins. We can represent\n",
    "these hypotheses in terms of what they say about the coin bias parameter.\n",
    "That is, we can represent them as *weights* or *priors*. We'll learn more\n",
    "in the next section about how to specify these, but for now I'll just\n",
    "generate some plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig.height: 7\n",
    "\n",
    "fair_coin <- plot(bayesplay::prior(\"point\", 0.5)) +\n",
    "  theme_minimal(14) +\n",
    "  labs(x = \"coin bias\", y = \"weight\")\n",
    "\n",
    "more_heads <- plot(bayesplay::prior(\"beta\", 2, 1)) +\n",
    "  theme_minimal(14) +\n",
    "  labs(x = \"coin bias\", y = \"weight\")\n",
    "\n",
    "more_tails <- plot(bayesplay::prior(\"beta\", 1, 2)) +\n",
    "  theme_minimal(14) +\n",
    "  labs(x = \"coin bias\", y = \"weight\")\n",
    "\n",
    "just_off <- plot(bayesplay::prior(\"beta\", 10, 10)) +\n",
    "  theme_minimal(14) +\n",
    "  labs(x = \"coin bias\", y = \"weight\")\n",
    "\n",
    "all_equal <- plot(bayesplay::prior(\"beta\", 1, 1)) +\n",
    "  theme_minimal(14) +\n",
    "  labs(x = \"coin bias\", y = \"weight\")\n",
    "\n",
    "(more_heads /\n",
    "  more_tails /\n",
    "  just_off) + plot_annotation(tag_levels = \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4fa6d",
   "metadata": {},
   "source": [
    "In panel **A**, values of the coin bias closer to 1 (show heads all the time)\n",
    "and given more weight than values closer to 0 (never show heads). This means we\n",
    "except the coin to show heads more often. In panel **B**, we see the opposite.\n",
    "Finally, in panel **C**, we weight values closer to 0.5 (fair) higher than\n",
    "values closer to 0 or 1. That is, we don't think the coin bias is exactly 0.5, \n",
    "but we think values closer to 0.5 are more probable than values further away\n",
    "from 0.5.\n",
    "\n",
    "Now that we're taking a weighted average, our earlier formula before:\n",
    "\n",
    "$$\\int_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|\\mathbf{y})d(\\theta)$$\n",
    "\n",
    "Now just becomes:\n",
    "\n",
    "$$\\int_{\\theta\\in\\Theta}\\mathcal{L}(\\theta|\\mathbf{y})p(\\theta)d\\theta$$\n",
    "\n",
    "In words we'd read this as:\n",
    "\n",
    "> *The probability of obtaining our data under the specified model is equal to\n",
    "> the integral of the likelhood (the model of the data) multiplied by the prior \n",
    "> (the weights).*\n",
    "\n",
    "We might denote this as $p(Y|\\mathcal{M}_i$) or simply $\\mathcal{M}_i$. When\n",
    "comparing two models---for example, $\\mathcal{M}_1$ and $\\mathcal{M}_0$, we\n",
    "take the ratio as follows:\n",
    "\n",
    "$$\\frac{\\mathcal{M}_1}{\\mathcal{M}_0}$$\n",
    "\n",
    "Try not to be too intimidated by the formula above. It just means that we're\n",
    "working out the probability of obtaining our data for a given value of the\n",
    "parameter, and that we're doing this for a range of parameter values. And\n",
    "finally, we're taking a weighted average of these. There's only 3 parts to the\n",
    "formula. \n",
    "\n",
    "1. The likelihood, which tells us the probability of obtained our data at\n",
    "   a specific value of the parameter: $\\mathcal{L}(\\theta|\\mathbf{y})$\n",
    "\n",
    "2. The prior, which determines the weights for weighted average of the \n",
    "   likelihood values: $p(\\theta)$\n",
    "\n",
    "3. The integral, which performs the \"averaging\" across all the different values\n",
    "   of the parameter range: $\\int_{\\theta\\in\\Theta}...d\\theta$\n",
    "\n",
    "#### Visualising predictions\n",
    "\n",
    "We can also represent these different models of the coin bais in terms of what\n",
    "outcomes we'd predict, just like we did with the earlier predictions. In the\n",
    "next section, we'll also learn about how to turns priors into predictions, but\n",
    "for now we'll just look at some plots. In each of these plots we'll show what\n",
    "we would  predict if the coin was exactly fair overlaid on each of these\n",
    "different models of *unfairness*. In each of the plots we'll highlight our\n",
    "actual outcome of 8 heads in 10 flips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67759e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| fig.height: 10\n",
    "\n",
    "`fair coin` <- bayesplay::extract_predictions(\n",
    "  bayesplay::likelihood(\"binomial\", 8, 10) *\n",
    "    bayesplay::prior(\"point\", 0.5)\n",
    ")\n",
    "\n",
    "`shows more heads` <- bayesplay::extract_predictions(\n",
    "  bayesplay::likelihood(\"binomial\", 8, 10) *\n",
    "    bayesplay::prior(\"beta\", 2, 1)\n",
    ")\n",
    "\n",
    "`shows more tails` <- bayesplay::extract_predictions(\n",
    "  bayesplay::likelihood(\"binomial\", 8, 10) *\n",
    "    bayesplay::prior(\"beta\", 1, 2)\n",
    ")\n",
    "\n",
    "`just off fair` <- bayesplay::extract_predictions(\n",
    "  bayesplay::likelihood(\"binomial\", 8, 10) *\n",
    "    bayesplay::prior(\"beta\", 10, 10)\n",
    ")\n",
    "\n",
    "`all equal` <- bayesplay::extract_predictions(\n",
    "  bayesplay::likelihood(\"binomial\", 8, 10) *\n",
    "    bayesplay::prior(\"beta\", 1, 1)\n",
    ")\n",
    "\n",
    "((bayesplay::visual_compare(`fair coin`, `shows more heads`) +\n",
    "  labs(x = \"number of heads\", y = \"p(y)\") +\n",
    "  theme_minimal(12)) /\n",
    "  (bayesplay::visual_compare(`fair coin`, `shows more tails`) +\n",
    "    labs(x = \"number of heads\", y = \"p(y)\") +\n",
    "    theme_minimal(12)) /\n",
    "  (bayesplay::visual_compare(`fair coin`, `just off fair`) +\n",
    "    labs(x = \"number of heads\", y = \"p(y)\") +\n",
    "    theme_minimal(12)) /\n",
    "  (bayesplay::visual_compare(`fair coin`, `all equal`) +\n",
    "    labs(x = \"number of heads\", y = \"p(y)\") +\n",
    "    theme_minimal(12))) +\n",
    "  plot_annotation(tag_levels = \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7aa05b6",
   "metadata": {},
   "source": [
    "In panel **A**, we see the predictions from our fair coin model against our\n",
    "predictions from a model where the coin shows heads more often. In panel **B**,\n",
    "we see the fair coin predictions against a coin that shows tails more often. In\n",
    "panel **C** we see our fair coin model again a model where the bias is just\n",
    "slightly off from fair. And finally in panel **D**, we see the predictions of\n",
    "the fair coin model against a model where we have no reason for thinking that\n",
    "one outcome is more likely than any other outcome. \n",
    "\n",
    "In each of these panels we can weigh up the evidence for whether our data\n",
    "support one model over the other by looking at whether our data would be\n",
    "produced more often if $\\mathcal{H}_1$ were true than if $\\mathcal{H}_2$ were\n",
    "true. That is,w we can see whether the data provide support for $\\mathcal{H}_1$\n",
    "over $\\mathcal{H}_2$ just by looking at whether the blue highlighted point is\n",
    "higher (more probable) than the red highlighted point (less probable)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

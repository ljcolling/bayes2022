{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f0c82",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "name": "setup",
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = FALSE)\n",
    "suppressMessages(expr =  {\n",
    "  if (\"xfun\" %in% row.names(installed.packages()) == FALSE) {\n",
    "    install.packages(\"xfun\")\n",
    "  }\n",
    "\n",
    "display_markdown <- \\(x) IRdisplay::display_markdown(as.character(x))\n",
    "display_html <- \\(x) IRdisplay::display_html(as.character(x))\n",
    "\n",
    "xfun::pkg_attach(\n",
    "    c(\"tidyverse\",\n",
    "      \"polspline\",\n",
    "      \"patchwork\",\n",
    "      \"magrittr\",\n",
    "      \"bayesplay\",\n",
    "      \"knitr\",\n",
    "      \"broom\",\n",
    "      \"bayesplay\"),\n",
    "      install = TRUE)\n",
    "\n",
    "})\n",
    "\n",
    "table_format <- \"html\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd79e02",
   "metadata": {},
   "source": [
    "# Moving beyond coin flips\n",
    "\n",
    "In the previous section (see [An alternative to *p*\n",
    "values](an-alternative-to-p-values.html), and [The Bayes\n",
    "factor](the-bayes-factor.html)), we were introduced to the concept of the\n",
    "**likelihood**. In these sections, we specifically covered the **binomial**\n",
    "likelihood, which can be used for working out Bayes factors for samples of\n",
    "Bernoulli trials---that is, trials with two discrete outcomes like heads and\n",
    "tails or successes and failures. We used this specifically for computing our\n",
    "Bayes factors for hypotheses about coin flips. Although these Bayes factors\n",
    "could be used anywhere where we might ordinarily use a frequentist **binomial\n",
    "test** it is still rather limited in scope. Therefore, in this section, we'll\n",
    "cover Bayes factors that can be used in other situations. Specifically, we'll\n",
    "focus primarily on situations where we're interested in **differences between\n",
    "means**---that is, situations where we might otherwise use a *t*-test or ANOVA.\n",
    "\n",
    "## Choosing a likelihood\n",
    "\n",
    "In our initial example on hunting treasure (see [Null hypothesis significance\n",
    "testing](null-hypothesis-significance-testing.html)) our treasure hunting\n",
    "device worked by, on average, pointing at 0 when there was no treasure\n",
    "around and, on average, pointing at some other value when there was treasure\n",
    "around.\n",
    "\n",
    "To work out whether our device was, on average, pointing at a particular value\n",
    "we collected a sample of a fixed size (10 in our first example) and then worked\n",
    "out the average of these values. For example, we might've collected a sample of\n",
    "10 values as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5043e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(123)\n",
    "\n",
    "sample_size <- 10\n",
    "sample_mean <- 0\n",
    "sample_sd <- 25\n",
    "sample_data <- rnorm(n = sample_size, m = sample_mean, sd = sample_sd)\n",
    "\n",
    "paste0(\"**Sample data**: \", paste(round(sample_data, 1), collapse = \", \"), \"\n",
    "\n",
    "**Sample mean**: \", round(mean(sample_data), 2)) %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c710b20",
   "metadata": {},
   "source": [
    "After collecting a large number of samples, we could plot our averages as\n",
    "follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d409a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generate_data <- function(size, mean, sd, samples) {\n",
    "  purrr::map_df(1:samples, function(x) {\n",
    "    data <- rnorm(size, mean, sd)\n",
    "    tibble::tibble(\n",
    "      mean = mean(data),\n",
    "      t = mean / (sd(data) / sqrt(size)),\n",
    "      d = mean / sd(data)\n",
    "    )\n",
    "  })\n",
    "}\n",
    "\n",
    "\n",
    "sample_means_data <- generate_data(10, 0, 25, 10000)\n",
    "sample_means_data_nonzero <- generate_data(10, 19.6, 25, 10000)\n",
    "\n",
    "\n",
    "generate_plot <- function(data) {\n",
    "  density_plot_means <- tibble(x = seq(min(data$mean),\n",
    "    max(data$mean),\n",
    "    length.out = 10000\n",
    "  )) %>%\n",
    "    mutate(y = polspline::dlogspline(\n",
    "      x,\n",
    "      polspline::logspline(data$mean)\n",
    "    ))\n",
    "\n",
    "  density_plot_ts <- tibble(x = seq(min(data$t),\n",
    "    max(data$t),\n",
    "    length.out = 10000\n",
    "  )) %>%\n",
    "    mutate(y = polspline::dlogspline(\n",
    "      x,\n",
    "      polspline::logspline(data$t)\n",
    "    ))\n",
    "\n",
    "  density_plot_ds <- tibble(x = seq(min(data$d),\n",
    "    max(data$d),\n",
    "    length.out = 10000\n",
    "  )) %>%\n",
    "    mutate(y = polspline::dlogspline(\n",
    "      x,\n",
    "      polspline::logspline(data$d)\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "  mean_plot <- ggplot() +\n",
    "    geom_histogram(\n",
    "      data = data,\n",
    "      aes(x = mean, y = ..density..), bins = 30, na.rm = TRUE\n",
    "    ) +\n",
    "    geom_line(\n",
    "      data = density_plot_means, aes(x = x, y = y), size = 1,\n",
    "      na.rm = TRUE\n",
    "    ) +\n",
    "    theme_minimal(14) +\n",
    "    labs(x = \"μ\", y = \"Density\") +\n",
    "    NULL\n",
    "\n",
    "  t_plot <- ggplot() +\n",
    "    geom_histogram(\n",
    "      data = data,\n",
    "      aes(x = t, y = ..density..), bins = 30, na.rm = TRUE,\n",
    "    ) +\n",
    "    geom_line(\n",
    "      data = density_plot_ts, aes(x = x, y = y), size = 1,\n",
    "      na.rm = TRUE\n",
    "    ) +\n",
    "    theme_minimal(14) +\n",
    "    labs(x = \"t\", y = \"Density\") +\n",
    "    NULL\n",
    "\n",
    "\n",
    "  d_plot <- ggplot() +\n",
    "    geom_histogram(\n",
    "      data = data,\n",
    "      aes(x = d, y = ..density..), bins = 30, na.rm = TRUE,\n",
    "    ) +\n",
    "    geom_line(\n",
    "      data = density_plot_ds, aes(x = x, y = y), size = 1,\n",
    "      na.rm = TRUE\n",
    "    ) +\n",
    "    theme_minimal(14) +\n",
    "    labs(x = \"δ\", y = \"Density\") +\n",
    "    NULL\n",
    "\n",
    "  list(mean_plot = mean_plot, t_plot = t_plot, d_plot = d_plot)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d016e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "  plot = generate_plot(sample_means_data),\n",
    "  limits = list(c(-30, 30), c(-4, 4), c(-2, 2))\n",
    ") %>%\n",
    "  pmap(function(plot, limits) plot + xlim(limits)) %>%\n",
    "  wrap_plots(ncol = 3) + plot_annotation(tag_level = \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a241621b",
   "metadata": {},
   "source": [
    "In the above examples, we've set the *parameter* of interest (the mean) to 0.\n",
    "The above plots are just examples of the sampling distribution when the\n",
    "parameter is 0. Panel **A** shows the sampling distribution of the *raw means*.\n",
    "Panel **B** shows the sampling distribution of the raw means re-scaled to *t*\n",
    "values. Finally, panel **C** shows the sampling distribution of the raw means\n",
    "re-scaled to *Cohen's d* values (where $\\delta = \\frac{\\mu}{\\sigma}$). All\n",
    "these plots approximately follow the shape of a *normal distribution* or a\n",
    "*t distribution*. We could use these plots to work out *p* values, just\n",
    "as we previously did.\n",
    "\n",
    "But, as we did with the coin flip examples, when working with *likelihoods*,\n",
    "we're interested in the probability of obtaining our data under different\n",
    "values of the parameter. That is, we must consider the probability of obtaining\n",
    "our current data not just in the case where the parameter of interest is 0, but\n",
    "also where the parameter of interest is some other value. For example, the\n",
    "plots below have been generated by setting the parameter value--that is, the\n",
    "value to which the device, on average points---to a raw mean of approximately\n",
    "19.6. We've also set the average spread of the values---that is, the standard\n",
    "deviation--to 25. Consequently, when rescaled to a *t* value, this would result\n",
    "in an average *t* of $\\frac{19.6}{\\frac{25}{\\sqrt{10}}}$. And when rescaled to\n",
    "a *d* value, this would result in an average *d* value of $\\frac{19.6}{25}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bedae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(\n",
    "  plot = generate_plot(sample_means_data_nonzero),\n",
    "  limits = list(c(-10, 50), c(-2, 8), c(-1, 5))\n",
    ") %>%\n",
    "  pmap(function(plot, limits) plot + xlim(limits)) %>%\n",
    "  wrap_plots(ncol = 3) + plot_annotation(tag_level = \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5f4b5",
   "metadata": {},
   "source": [
    "For these plots, panel **A** again shows the *raw means*, with panel **B**\n",
    "showing the *t* values, and panel **C** showing the Cohen's *d* values. The\n",
    "distribution in Panel **A** now approximately follows the shape of a *normal\n",
    "distribution* or a *scaled and shifted t distribution*. Panel **B** and **C**\n",
    "now follow the shape of slightly differently scaled versions of the\n",
    "*non-central t distribution*.\n",
    "\n",
    "In the section that follows, we'll discover and learn how to use these\n",
    "likelihoods: The *normal* likelihood, the *scaled and shifted t* likelihood,\n",
    "and three versions of the *non-central t* likelihood (which `bayesplay` calls\n",
    "the *non-central t*, *non-central d*, and *non-central d2* likelihoods).\n",
    "\n",
    "### The variance of likelihoods\n",
    "\n",
    "When we were examining coin flips, we saw that the sampling distribution (in\n",
    "the flip until **n** flips case) followed the *binomial distribution*. And when\n",
    "we wanted to make inferences about parameter values (the coin bias) we used the\n",
    "*binomial* likelihood. Our data, which we used to make our inference consisted\n",
    "of, first, the number of heads and, second, the number of flips.\n",
    "\n",
    "These two values controlled different aspects of the shape of the likelihood\n",
    "function. First, the number of heads that we observed more-or-less controlled\n",
    "where the peak of the likelihood function was located. Second, the number of\n",
    "flips, or the sample size, controlled how spread out the likelihood function\n",
    "was. In the plots below we can see three cases of observing $\\frac{n}{2}$ heads\n",
    "in 2, 20, and 100 flips. In all three plots, the likelihood function is peaked\n",
    "at 0.5 and drops off as we move away from 0.5. The rate of this drop off,\n",
    "however, is steeper as the sample size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a26e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihoods <- c(\n",
    "  likelihood(\"binomial\", 2 / 2, 2),\n",
    "  likelihood(\"binomial\", 20 / 2, 20),\n",
    "  likelihood(\"binomial\", 100 / 2, 100)\n",
    ")\n",
    "\n",
    "(map(likelihoods, function(x) {\n",
    "  plot(x) +\n",
    "    theme_minimal(14) +\n",
    "    geom_vline(xintercept = 0.8, linetype = 2) +\n",
    "    scale_x_continuous(\n",
    "      breaks = c(0, 0.2, .5, 0.8, 1),\n",
    "      limits = c(0, 1)\n",
    "    ) +\n",
    "    labs(subtitle = glue::glue(\n",
    "      \"{x$parameters[[1]]} heads in {x$parameters[[2]]}\"\n",
    "    ))\n",
    "}) %>%\n",
    "  wrap_plots(ncol = 3) + plot_annotation(tag_level = \"A\")) %>% show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2d5a5",
   "metadata": {},
   "source": [
    "What is the important intuition here? To get a handle on the intuition we can\n",
    "think of the extreme cases. When the coin is very biased---for example, it\n",
    "shows heads 0.8 of the time---then it will still sometimes show tails. It might\n",
    "even sometimes show tails on the first flip. In fact, it'll show tails on the\n",
    "first flip 0.2 of the time. Therefore, we wouldn't be that surprised if after\n",
    "two flips we have one head and one tail, because it won't be such an uncommon\n",
    "occurrence. But in the situation where we're making 100 flips, it now becomes\n",
    "more and more unlikely that we'd see equal numbers of heads and tails if the\n",
    "coin bias really was 0.8. We can put numbers to it by calculating the\n",
    "likelihood ratio between 0.5 and 0.8 for each of the three sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f55dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_df(likelihoods, function(x) {\n",
    "  tibble(\n",
    "    lr = x$likelihood_function(0.5) / x$likelihood_function(0.8),\n",
    "    trials = x$parameters$trials\n",
    "  )\n",
    "}) %>%\n",
    "  glue::glue_data(\"When there are {trials} trials, the likelihood ratio\n",
    "between θ = 0.5 and θ = 0.8 is {round(lr,2)}.\\n\\n\\n\") %>%\n",
    "  paste0(., collapse = \"\\n\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c09348",
   "metadata": {},
   "source": [
    "For all the likelihoods that we'll examine in this section, when defining the\n",
    "likelihoods we'll have one value that represents our observation: the mean we\n",
    "observe, the *t* value we observe, or the *d* value we observe. This will be\n",
    "analogous to the number of heads we observe in the coin flip example. And we'll\n",
    "have a value (or values) that defines how peaked or spread out the likelihood\n",
    "will be: this could be the sample size, the degrees of freedom and/or standard\n",
    "deviation. \n",
    "\n",
    "In more technical terms, all the likelihoods that we'll examine will have one\n",
    "parameter that we're making inferences about: the *mean*, the *t* value, or the\n",
    "*d* value. But this parameter will also have a *variance* associated with it.\n",
    "And as we'll see in the examples below, depending on whether we're interested\n",
    "in *raw means*,*t* values, or *d* values, the sampling distributions are\n",
    "slightly different shapes. Therefore, the *likelihoods* that we use in each\n",
    "case will be slightly different. We'll look at each of these in turn.\n",
    "\n",
    "## Inferences about raw means\n",
    "\n",
    "When we're interested in making inferences about the *raw means* we have two\n",
    "choices available to us. The most straightforward choice is to choose the\n",
    "*normal* likelihood. The shape of the *normal* likelihood is controlled by\n",
    "**two** values. The first value is our **observed mean**. This value controls\n",
    "the location of the peak of the likelihood function. The second value is the\n",
    "**standard deviation of the mean**.  The **standard deviation of the mean** is\n",
    "more commonly known as the **standard error of the mean**. We can work out the\n",
    "*standard error* using the following formula:\n",
    "\n",
    "$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}},$$\n",
    "\n",
    "where $\\sigma$ is the standard deviation of the population. Usually, we don't\n",
    "know the value of $\\sigma$, so we estimate it using $s$, or the standard\n",
    "deviation of our sample.\n",
    "\n",
    "To see how defining a *normal* likelihood works in practice, we'll generate our\n",
    "data. From this, we'll work out the mean of our sample, and we'll estimate the\n",
    "standard error (the standard deviation of the mean).\n",
    "\n",
    "To generate our data, we'll set up a data generating process (you can think of\n",
    "this as **the population**) and we'll draw a sample of 10 values from this. Our\n",
    "data generating process will have a $\\mu$ (mean of the population) of 19.6, and\n",
    "a $\\sigma$ (standard deviation of the population) of 25.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd81c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set.seed(123)\n",
    "sample_data <- rnorm(n = 10, mean = 19.6, sd = 25)\n",
    "\n",
    "glue::glue(\"**Sample data**: {paste(round(sample_data, 2), collapse = ', ')}\n",
    "\n",
    "**Mean of sample**: {round(mean(sample_data), 2)}\n",
    "\n",
    "\n",
    "**Standard deviation of sample**: {round(sd(sample_data), 2)}\n",
    "\n",
    "\n",
    "**Size of sample**: {round(length(sample_data), 2)}\n",
    "\n",
    "**Standard deviation of sampling distribution (standard error)**: {round(sd(sample_data)/sqrt(length(sample_data)) ,2)}\n",
    "\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d23dd0",
   "metadata": {},
   "source": [
    "Note that the two values that we want are the *mean of the sample* and the\n",
    "*standard deviation of the mean*. A common confusion is that you want the\n",
    "*standard deviation of the sample*. This is not the value that we want, and we\n",
    "only calculate it because we can use it to estimate the *standard deviation of\n",
    "the population* and, from this, the *standard deviation of the mean*.\n",
    "\n",
    "Now we can define the likelihood, and we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f102f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "data_model <- likelihood(\n",
    "  family = \"normal\",\n",
    "  mean = 21.47,\n",
    "  sd = 7.54\n",
    ")\n",
    "\n",
    "plot(data_model) +\n",
    "  labs(title = \"likelihood for mean = 21.47 (normal)\") +\n",
    "  xlim(-10, 50) +\n",
    "  theme_minimal(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b8ce0",
   "metadata": {},
   "source": [
    "From our likelihood plot we can see that our data would be generated more often\n",
    "if the mean of the data generating process was 21 and less often if the mean of\n",
    "the data generating process was 30.\n",
    "\n",
    "In fact, we can put a number to it and say that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81828e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "likelihood_ratio_n <- data_model$likelihood_function(21) /\n",
    "  data_model$likelihood_function(30)\n",
    "\n",
    "glue::glue(\"The data would be produced {round(likelihood_ratio_n, 2)} times more\n",
    "often if the mean of the population was 21 than it would be if the mean of the\n",
    "population was 30.\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edcd3bc",
   "metadata": {},
   "source": [
    "The likelihood function will get wider or narrower when the *standard deviation\n",
    "of the mean* changes. The two factors that control the *standard deviation of\n",
    "the mean* are the sample size and the standard deviation of the population. In\n",
    "the example below we'll keep our estimate of the *standard deviation of the\n",
    "population* the same but we'll increase the sample size. Consequently, the\n",
    "*standard deviation of the mean* will decrease and our likelihood function will\n",
    "become narrower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ad828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "data_model <- likelihood(\n",
    "  family = \"normal\",\n",
    "  mean = 21.47,\n",
    "  sd = 2.384\n",
    ")\n",
    "\n",
    "plot(data_model) +\n",
    "  labs(title = \"likelihood for mean = 21.47\") +\n",
    "  xlim(-10, 50) +\n",
    "  theme_minimal(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee1fbef",
   "metadata": {},
   "source": [
    "And this would also make a change to any likelihood ratio we could calculate.\n",
    "For example, calculating the new likelihood ratio comparing $\\mu = 30$ and $\\mu\n",
    "= 21$ would give the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a39abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ratio <- data_model$likelihood_function(21) /\n",
    "  data_model$likelihood_function(30)\n",
    "\n",
    "glue::glue(\"The data would be produced {round(likelihood_ratio, 2)} times more\n",
    "often if the mean of the population was 21 than it would be if the mean of the\n",
    "population was 30.\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d186f920",
   "metadata": {},
   "source": [
    "In the preceding examples, we were modelling our data with a *normal*\n",
    "likelihood. And we were estimating the *standard deviation of the mean* using\n",
    "the *standard deviation of our sample*. We did this, because we didn't know the\n",
    "actual standard deviation of our data generating process. We'd need to know\n",
    "this value if we wanted to exactly calculate the *standard deviation of the\n",
    "mean*.\n",
    "\n",
    "Or at least, we ordinarily don't know the standard deviation of our data\n",
    "generating process. However, because I set it up, I know it is 25, because this\n",
    "is the value I set it to. Therefore, our estimate was an under estimate.\n",
    "Typically, we'll underestimate the *standard deviation of the population*.\n",
    "These under-estimates will be worse when the sample size is small. As our\n",
    "sample size increases then the two values will, by definition, match.\n",
    "\n",
    "There are a few approaches that we can take to dealing with this issue. First,\n",
    "we can just do nothing. This is the most straightforward approach, and it is\n",
    "also a very common approach. The second approach, is to apply a correction as\n",
    "follows:\n",
    "\n",
    "$$s_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\times \\left(1 + \\frac{20}{\\mathrm{df}^2}\\right),$$\n",
    "\n",
    "where $\\mathrm{df}$ are the degrees of freedom for the corresponding *t* test.\n",
    "[Dienes (2014, p 11)](http://dx.doi.org/10.3389/fpsyg.2014.00781) provides\n",
    "more details on this approach. Dienes (2014) also provides good guidance on\n",
    "using Bayes factors, and I would recommend reading it for the assessment.\n",
    "\n",
    "Finally, the third approach, is to employ a *scaled and shifted t likelihood*\n",
    "instead of a *normal likelihood*. The *scaled and shifted t likelihood* has\n",
    "fatter tails than the *normal likelihood*, which accounts for the fact that our\n",
    "*normal likelihood* tends to be narrower than it ought to be.\n",
    "\n",
    "In `bayesplay` we can use the *scaled and shifted t likelihood* by setting the\n",
    "likelihood **family** to **student_t**. When using this likelihood, one\n",
    "additional value will need to be set. This is the **df** value, which is the \n",
    "same as the **df** value that we would use for the correction approach above.\n",
    "\n",
    "We'll recompute our previous example using this likelihood family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fa404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "data_model <- likelihood(\n",
    "  family = \"student_t\",\n",
    "  mean = 21.47,\n",
    "  sd = 7.54,\n",
    "  df = 9\n",
    ")\n",
    "\n",
    "plot(data_model) +\n",
    "  labs(title = \"likelihood for mean = 21.47 (student t)\") +\n",
    "  xlim(-10, 50) +\n",
    "  theme_minimal(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa32874",
   "metadata": {},
   "source": [
    "As you can see the two likelihood functions look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_ratio_t <- data_model$likelihood_function(21) /\n",
    "  data_model$likelihood_function(30)\n",
    "\n",
    "glue::glue(\"The data would be produced {round(likelihood_ratio_t, 2)} times more\n",
    "often if the mean of the population was 21 than it would be if the mean of the\n",
    "population was 30.\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b81d8",
   "metadata": {},
   "source": [
    "This likelihood ratios are also very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f128c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "likelihood_ratio_diff <- abs(likelihood_ratio_t - likelihood_ratio_n)\n",
    "\n",
    "glue::glue(\"The difference in the likelihood ratio between the *normal*\n",
    "likelihood and the *student_t* likelihood is about\n",
    "{round(likelihood_ratio_diff,2)}\") %>%\n",
    "  display_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09d5a3",
   "metadata": {},
   "source": [
    "## Inferences about effect sizes\n",
    "\n",
    "A very popular alternative approach to modelling data in terms of the observed\n",
    "mean is to instead model data in terms of standardized effect sizes on the raw\n",
    "means themselves. This gets around the problem of the unknown variance;\n",
    "however, it has an added benefit in that it can place results from very\n",
    "different experiments on a common scale. For example, results from a study of\n",
    "reaction times might have values between 500 and 2000 ms and results from a\n",
    "study on test scores might have values that range between 40 and 90. By\n",
    "re-scaling mean values to *standardised means*---that is, to effect sizes---we\n",
    "can be more certain that values will fall somewhere between -10 / 10, and more\n",
    "typically between -1/1. When we talk more about priors, we'll see that this\n",
    "rescaling will help us to come up with priors that will work in a wide range of\n",
    "settings and with a wide range of experiments and types of data. However, we'll\n",
    "also see that when we want to come up with priors for specific situations,\n",
    "thinking in terms of **standardised effects** rather than actual differences in\n",
    "data can get confusing.\n",
    "\n",
    "To define a likelihood based on effect size, we first need to work out the\n",
    "effect size. There are two formulas for effect sizes, depending on whether\n",
    "we have data from one group (or from paired samples) or whether we have data\n",
    "from two groups.\n",
    "\n",
    "If we have data from paired samples, then we first work out the pair-wise\n",
    "differences. Following this, we proceed as we would for the one-sample case. To\n",
    "work out the effect size for the one sample case, we use the following formula:\n",
    "\n",
    "$$d = \\frac{m}{s},$$\n",
    "\n",
    "Where $m$ is the mean of the sample, and $s$ is the standard deviation of the\n",
    "sample.\n",
    "\n",
    "If we have data from two groups then the formula is a little more complex. For\n",
    "the two group case, the formula is as follows:\n",
    "\n",
    "$$d = \\frac{m_1 - m2}{s_\\mathrm{pooled}},$$\n",
    "\n",
    " where $s_\\mathrm{pooled}$ is given as follows:\n",
    "\n",
    "$$s_\\mathrm{pooled} = \\sqrt{\\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2 -2}},$$\n",
    "\n",
    "and where $m_1$/$m_2$, $s_1$/$s_2$, and $n_1$/$n_2$, are the mean, standard\n",
    "deviation, and sample size for group 1 and group 2. In the worked examples\n",
    "below we'll see that we can use the `effsize` package in R to work out this\n",
    "effect size.\n",
    "\n",
    "Once we have the effect size, then the only other value that we need is the\n",
    "sample size. We have one sample size in the one group case (sample size or\n",
    "number of pairs), and for the two group case we'll have the sample size of each\n",
    "group. We'll walk through a couple of examples using simulated data.\n",
    "\n",
    "In the following example, we have data from an experiment looking at memory for\n",
    "words. The words were presented under two conditions: an *emotional* condition\n",
    "and a *neutral* condition. We're interested in knowing whether there is a\n",
    "difference in recognition accuracy between the two conditions. This is the kind\n",
    "of data we'd ordinarily analyse with a *t* test.\n",
    "\n",
    "We'll load the data, work out the effect size, define the likelihood, and then\n",
    "plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ccdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "# First load the data\n",
    "word_data <- readr::read_csv(\n",
    "  \"https://files.mindsci.net/word_data.csv\",\n",
    "  show_col_types = FALSE\n",
    ")\n",
    "\n",
    "# Now we'll work out the effect size and n\n",
    "summary_data <- word_data %>%\n",
    "  pivot_wider(1:3,\n",
    "    names_from = \"condition\",\n",
    "    values_from = \"accuracy\"\n",
    "  ) %>%\n",
    "  mutate(diff = emotional - neutral) %>%\n",
    "  summarise(m = mean(diff), s = sd(diff), n = n()) %>%\n",
    "  mutate(d = m / s, t = m / (s / sqrt(n)))\n",
    "\n",
    "\n",
    "# Now define the likelihood and plot it\n",
    "data_model <- likelihood(\n",
    "  family = \"noncentral_d\",\n",
    "  d = summary_data$d,\n",
    "  n = summary_data$n\n",
    ")\n",
    "\n",
    "plot(data_model) +\n",
    "  labs(\n",
    "    x = \"δ\", y = \"P(Y|θ)\",\n",
    "    title = \"Likelihood for:\",\n",
    "    subtitle = glue::glue(\n",
    "      \"d = {round(summary_data$d,2)},\",\n",
    "      \" n = {summary_data$n}\"\n",
    "    )\n",
    "  ) +\n",
    "  theme_minimal(14) +\n",
    "  scale_x_continuous(limits = c(-1, 1), breaks = seq(-1, 1, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa0c9b4",
   "metadata": {},
   "source": [
    "In the second example, we'll look at the two group case. For this example we'll\n",
    "use some simulated data to match some data from an experiment I conducted many\n",
    "years ago. In this task, people were asked to watch an animated avatar\n",
    "performing a movement, they were asked to synchronise a button press with\n",
    "critical points in the movement, and the timing error was measured. The\n",
    "animated avatar moved in two difference ways. In one condition it moved like a\n",
    "human. In the other condition, the dynamics of the movement were altered so\n",
    "that it moved like a robot. All participants viewed both kinds of movements. In\n",
    "addition to this within subjects factor, there was also a between subjects\n",
    "factor. Before viewing any of the movement, participants were split into two\n",
    "groups. One group was given experience actually performing the movement they\n",
    "would later observe, while the other group was not.\n",
    "\n",
    "This is the kind of data that would ordinarily be analysed using a 2 × 2 mixed\n",
    "ANOVA. However, I was particularly interested in the **interaction**. The\n",
    "**interaction** just examines whether the difference between **condition 1**\n",
    "and **condition 2** is different between **group 1** and **group 2**.  In the\n",
    "example below, I've already worked out the difference in the timing error for\n",
    "**condition 1** and **condition 2**, and now we just have to compare this\n",
    "difference between the two groups.\n",
    "\n",
    "As with the earlier example, we'll load the data, work out the effect size, \n",
    "define the likelihood, and then plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dfbf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "# First load the data\n",
    "motor_exp <- readr::read_csv(\n",
    "  \"https://files.mindsci.net/motor_exp.csv\",\n",
    "  show_col_types = FALSE\n",
    ")\n",
    "\n",
    "\n",
    "# Now we'll work out the effect size and n\n",
    "summary_data <- motor_exp %>%\n",
    "  dplyr::group_by(group) %>%\n",
    "  dplyr::summarise(m = mean(r_diff), s = sd(r_diff), n = n()) %>%\n",
    "  tidyr::pivot_wider(\n",
    "    names_from = \"group\",\n",
    "    values_from = c(\"m\", \"s\", \"n\")\n",
    "  )\n",
    "\n",
    "md_diff <- summary_data$m_exper - summary_data$m_naive\n",
    "sd_pooled <- sqrt((((summary_data$n_exper - 1) *\n",
    "  summary_data$s_exper^2) +\n",
    "  ((summary_data$n_naive - 1) * summary_data$s_naive^2)) /\n",
    "  (summary_data$n_exper + summary_data$n_naive - 2))\n",
    "d <- md_diff / sd_pooled\n",
    "\n",
    "# or we can use the effsize package\n",
    "# you'll just need to install it before you use it\n",
    "# you can install it with the following command\n",
    "# install.package(\"effsize\")\n",
    "#\n",
    "# and then use it as follows\n",
    "# d <- effsize::cohen.d(\n",
    "#   motor_exp$r_diff,\n",
    "#   motor_exp$group\n",
    "# )$estimate\n",
    "\n",
    "# sample_size <- motor_exp %>%\n",
    "#   dplyr::group_by(group) %>%\n",
    "#   dplyr::summarise(n = n())\n",
    "\n",
    "# Now define the likelihood and plot it\n",
    "data_model <- likelihood(\n",
    "  family = \"noncentral_d2\",\n",
    "  d = d,\n",
    "  n1 = summary_data$n_exper,\n",
    "  n2 = summary_data$n_naive\n",
    ")\n",
    "\n",
    "plot(data_model) +\n",
    "  labs(\n",
    "    x = \"δ\", y = \"P(Y|θ)\",\n",
    "    title = \"Likelihood for:\",\n",
    "    subtitle = glue::glue(\n",
    "      \"d = {round(d,2)},\",\n",
    "      \" n1 = {summary_data$n_exper},\",\n",
    "      \" n2 = {summary_data$n_naive}\"\n",
    "    )\n",
    "  ) +\n",
    "  theme_minimal(14) +\n",
    "  scale_x_continuous(limits = c(-1, 3), breaks = seq(-1, 3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959c8cc",
   "metadata": {},
   "source": [
    "## Inferences about *t* values\n",
    "\n",
    "Finally, we'll repeat the last analysis, but in this case we'll model the data\n",
    "in terms of *t* rather than *d*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1867bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo = TRUE, include = TRUE\n",
    "\n",
    "# Load the data again just in case\n",
    "motor_exp <- readr::read_csv(\n",
    "  \"https://files.mindsci.net/motor_exp.csv\",\n",
    "  show_col_types = FALSE\n",
    ")\n",
    "\n",
    "# Run the t test and extract the t value and df\n",
    "t_test_res <- t.test(r_diff ~ group,\n",
    "  motor_exp,\n",
    "  var.equal = TRUE\n",
    ") %>%\n",
    "  broom::tidy() %>%\n",
    "  dplyr::select(statistic, parameter)\n",
    "\n",
    "# Now define the likelihood\n",
    "\n",
    "data_model <- likelihood(\n",
    "  family = \"noncentral_t\",\n",
    "  t = t_test_res$statistic,\n",
    "  df = t_test_res$parameter\n",
    ")\n",
    "\n",
    "# And plot it\n",
    "plot(data_model) +\n",
    "  labs(\n",
    "    x = \"t\", y = \"P(Y|θ)\",\n",
    "    title = \"Likelihood for:\",\n",
    "    subtitle = glue::glue(\n",
    "      \"t = {round(t_test_res$statistic,2)},\",\n",
    "      \" df = {t_test_res$parameter}\"\n",
    "    )\n",
    "  ) +\n",
    "  theme_minimal(14) +\n",
    "  scale_x_continuous(limits = c(-2, 6), breaks = seq(-2, 6, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8dfaf",
   "metadata": {},
   "source": [
    "Using the *non-central t* likelihood might seem a little easier to use, because\n",
    "it requires less work upfront because we can just use the `t.test` function to\n",
    "work out the *t* statistic instead of having to work out the *d* value.\n",
    "However, as we'll see in the section on **priors**, there as advantages to using\n",
    "the *non-central t* likelihood. This disadvantage is primarily to do with the\n",
    "fact that the *t* value can change dramatically with sample size---that is, \n",
    "very large sample sizes can result in very large *t* values even if the mean \n",
    "difference between conditions or groups stays constant. For the *non-central d* \n",
    "and *non-central d2* likelihoods this isn't an issue. The *d* value will stay\n",
    "the same even if the sample size increases and instead, the likelihood will \n",
    "just get narrower.\n",
    "\n",
    "I've covered the *non-central t* likelihood for completeness, but it's almost\n",
    "never of any real use.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
